import os
import random
import numpy as np
import pandas as pd
from PIL import Image

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW

from typing import List, Dict, Any, Tuple, Optional, Union

# ============================================================
# 0. ê³µí†µ ì„¤ì •
# ============================================================
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

TRAIN_CSV = "prompt_tuning_train_750.csv"
VAL_CSV   = "prompt_tuning_val_150.csv"

MODEL_ID_BY_BACKEND = {
    "qwen3":    "Qwen/Qwen3-VL-2B-Instruct",
    "medgemma": "google/medgemma-4b-it",
    "internvl": "OpenGVLab/InternVL3_5-2B-HF",
    "lingshu":  "lingshu-medical-mllm/Lingshu-7B",
}

PROMPT_BY_DATASET = {
    "mri": (
        "This is a brain MRI scan.\n"
        "Question: Does this image show normal anatomy or signs of disease?\n\n"
    ),
    "oct": (
        "This is a retinal OCT scan.\n"
        "Question: Does this image show normal anatomy or signs of disease?\n\n"
    ),
    "xray": (
        "This is a chest X-ray image.\n"
        "Question: Does this image show normal anatomy or signs of disease?\n\n"
    ),
    "fundus": (
        "This is a retinal fundus photograph.\n"
        "Question: Does this image show normal anatomy or signs of disease?\n\n"
    ),
}

SYSTEM_PROMPT_NORMAL = (
    "You are a medical image classifier.\n"
    "You must answer using ONLY ONE WORD:\n"
    "either \"normal\" or \"disease\".\n\n"
    "Do NOT include any other text, explanation, punctuation,\n"
    "formatting, or symbols. Output exactly one token."
)


def load_backend(backend, model_id):
    """
    backend íƒ€ì…ì— ë”°ë¼ base VLMê³¼ processor ë¡œë“œ
    """
    dtype = torch.bfloat16 if device == "cuda" else torch.float32

    if backend == "qwen3":
        from transformers import Qwen3VLForConditionalGeneration, AutoProcessor
        model = Qwen3VLForConditionalGeneration.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else {"": "cpu"},
        )
        processor = AutoProcessor.from_pretrained(model_id)
        model.eval()
        return model, processor

    if backend in ["medgemma", "internvl"]:
        from transformers import AutoProcessor
        processor = AutoProcessor.from_pretrained(model_id)
        try:
            from transformers import AutoModelForImageTextToText
            model = AutoModelForImageTextToText.from_pretrained(
                model_id,
                torch_dtype=dtype,
                device_map="auto" if device == "cuda" else {"": "cpu"},
            )
        except Exception:
            from transformers import AutoModelForVision2Seq
            model = AutoModelForVision2Seq.from_pretrained(
                model_id,
                torch_dtype=dtype,
                device_map="auto" if device == "cuda" else {"": "cpu"},
            )
        model.eval()
        return model, processor

    if backend == "lingshu":
        from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_id,
            torch_dtype=dtype,
            device_map="auto" if device == "cuda" else {"": "cpu"},
        )
        processor = AutoProcessor.from_pretrained(model_id)
        model.eval()
        return model, processor

    raise ValueError(f"Unknown backend: {backend}")


# ì§€ê¸ˆì€ InternVLë§Œ í•™ìŠµ
BACKENDS = ["internvl","lingshu"]
NUM_VIRTUAL_TOKENS = 20

# CE ê°€ì¤‘ì¹˜ (weakì— ë” í° weight)
w_c = 0.5   # clean
w_w = 1.5  # weak

# consistency ê°€ì¤‘ì¹˜
lambda_w = 0.5  # clean vs weak

LR = 1e-3
EPOCHS = 10
BATCH_SIZE_DEFAULT = 15
BATCH_SIZE_BY_BACKEND = {
    "internvl": 1,
    "lingshu": 2,
}

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if device == "cuda":
    torch.cuda.manual_seed_all(SEED)


# ============================================================
# 1. SoftPrompt ë˜í¼
# ============================================================
class SoftPromptVLM(nn.Module):
    """
    - base_model: frozen VLM
    - soft_prompt: ì•ì— ë¶™ëŠ” ê°€ìƒ í† í° ì„ë² ë”©
    - classifier: hidden_size -> 2 (normal / disease)
    """
    def __init__(self, base_model, num_virtual_tokens: int = 20, keep_input_ids: bool = False):
        super().__init__()
        self.base_model = base_model
        self.num_virtual_tokens = num_virtual_tokens
        self.keep_input_ids = keep_input_ids

        # ì…ë ¥ ì„ë² ë”©ì—ì„œ hidden size ì¶”ì¶œ
        input_embeddings = self.base_model.get_input_embeddings()
        hidden_size = input_embeddings.weight.shape[1]
        embed_dtype = input_embeddings.weight.dtype
        embed_device = input_embeddings.weight.device

        # soft prompt embedding
        self.soft_prompt = nn.Embedding(
            num_virtual_tokens,
            hidden_size,
            dtype=embed_dtype,
            device=embed_device,
        )

        # hidden -> 2-class classifier
        self.classifier = nn.Linear(
            hidden_size,
            2,
            device=embed_device,
            dtype=embed_dtype,
        )

        # base_model íŒŒë¼ë¯¸í„°ëŠ” freeze
        for p in self.base_model.parameters():
            p.requires_grad = False

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        labels=None,
        output_hidden_states: bool = False,
        **kwargs,
    ):
        if input_ids is None:
            raise ValueError("SoftPromptVLM.forward requires input_ids")

        # ì›ë³¸ í† í° ì„ë² ë”©
        input_embeds = self.base_model.get_input_embeddings()(input_ids)
        B, L, H = input_embeds.shape
        dev = input_embeds.device

        # dtype ë§ì¶”ê¸°
        target_dtype = self.soft_prompt.weight.dtype
        if input_embeds.dtype != target_dtype:
            input_embeds = input_embeds.to(target_dtype)

        # soft prompt ì„ë² ë”©
        virtual_token_ids = torch.arange(
            self.num_virtual_tokens, device=dev
        ).unsqueeze(0).expand(B, -1)  # (B, V)
        soft_embeds = self.soft_prompt(virtual_token_ids)  # (B, V, H)

        # [soft] + [original]
        inputs_embeds = torch.cat([soft_embeds, input_embeds], dim=1)  # (B, V+L, H)

        # attention mask í™•ì¥
        if attention_mask is not None:
            soft_mask = torch.ones(
                (B, self.num_virtual_tokens),
                device=dev,
                dtype=attention_mask.dtype,
            )
            attention_mask = torch.cat([soft_mask, attention_mask], dim=1)

        # labels í™•ì¥ (CausalLM lossëŠ” ì•ˆ ì“°ì§€ë§Œ, í˜¹ì‹œ labels ë“¤ì–´ì˜¤ë©´ ì•ˆì „í•˜ê²Œ)
        if labels is not None:
            pad = torch.full(
                (B, self.num_virtual_tokens),
                fill_value=-100,
                device=dev,
                dtype=labels.dtype,
            )
            labels = torch.cat([pad, labels], dim=1)

        # ì¼ë¶€ backendëŠ” kwargsì— input_ids í•„ìš” ì—†ìŒ
        if "input_ids" in kwargs and not self.keep_input_ids:
            kwargs.pop("input_ids")

        # hidden_states ì „ì²´ëŠ” ì•ˆ ì“°ê³ ,
        # last_hidden_state / encoder_last_hidden_state ìœ„ì£¼ë¡œ ì“¸ ê±°ë¼
        # output_hidden_states=False ìœ ì§€ (OOM ë°©ì§€)
        outputs = self.base_model(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            labels=labels,
            return_dict=True,
            output_hidden_states=True,  # ğŸ‘ˆ ì¶”ê°€
            **kwargs,
        )
        return outputs

    def extract_features(self, outputs: Any) -> torch.Tensor:
        """
        ë‹¤ì–‘í•œ output íƒ€ì…ì— ëŒ€ì‘í•´ì„œ (B, H) feature ë½‘ê¸°.

        ìš°ì„ ìˆœìœ„:
          1) outputs.last_hidden_state / outputs["last_hidden_state"]
          2) outputs.encoder_last_hidden_state / outputs["encoder_last_hidden_state"]
          3) outputs.hidden_states[-1]
          4) InternVLë¥˜ nested output (language_model_output ë“±)
          5) tuple/list ì—ì„œ (B, T, H) í…ì„œ ì¶”ë¡ 
        """

        def _has_attr_or_key(obj, name: str):
            if hasattr(obj, name) and getattr(obj, name) is not None:
                return getattr(obj, name)
            if isinstance(obj, dict) and name in obj and obj[name] is not None:
                return obj[name]
            return None

        # ---------- 1) last_hidden_state ----------
        hs = _has_attr_or_key(outputs, "last_hidden_state")
        if isinstance(hs, torch.Tensor):
            if hs.dim() == 3:
                return hs[:, -1, :]  # (B, H)
            elif hs.dim() == 2:
                return hs            # (B, H)

        # ---------- 2) encoder_last_hidden_state ----------
        enc = _has_attr_or_key(outputs, "encoder_last_hidden_state")
        if isinstance(enc, torch.Tensor):
            if enc.dim() == 3:
                return enc.mean(dim=1)  # (B, H)
            elif enc.dim() == 2:
                return enc

        # ---------- 3) hidden_states[-1] ----------
        hidden_states = _has_attr_or_key(outputs, "hidden_states")
        if isinstance(hidden_states, (list, tuple)) and len(hidden_states) > 0:
            hs_last = hidden_states[-1]
            if isinstance(hs_last, torch.Tensor):
                if hs_last.dim() == 3:
                    return hs_last[:, -1, :]
                elif hs_last.dim() == 2:
                    return hs_last

        # ---------- 4) InternVL ê³„ì—´ nested output ----------
        #   ì˜ˆ: outputs.language_model_output.last_hidden_state
        for key in ["language_model_output", "lm_output", "text_outputs"]:
            nested = _has_attr_or_key(outputs, key)
            if nested is not None:
                try:
                    return self.extract_features(nested)
                except ValueError:
                    pass  # ëª» ë½‘ìœ¼ë©´ ë‹¤ìŒ í›„ë³´ë¡œ

        # ---------- 5) tuple/list fallback ----------
        # return_dict=False ì¸ ëª¨ë¸ì´ë‚˜ remote codeì—ì„œ ìì£¼ ë°œìƒ.
        # (loss, logits, hidden_states) / (logits, hidden_states) / (logits,) ë“± ë‹¤ì–‘í•˜ë¯€ë¡œ
        # (B, T, H) í˜•íƒœë©´ì„œ hidden_size ìª½ì´ ìƒëŒ€ì ìœ¼ë¡œ ì‘ì€ í…ì„œë¥¼ ê³¨ë¼ ì”€.
        if isinstance(outputs, (list, tuple)):
            candidate = None
            for x in outputs:
                if isinstance(x, torch.Tensor) and x.dim() == 3:
                    # vocab_size(ë³´í†µ > 10k) ë§ê³  hidden_size(ë³´í†µ <= 4096) ìª½ì„ ì„ íƒ
                    if x.size(-1) <= 4096:
                        candidate = x
                        break
            if candidate is not None:
                return candidate[:, -1, :]

        raise ValueError("No usable hidden representations in outputs.")



# ============================================================
# 2. train/val CSV ë¡œë“œ (clean/weakë§Œ)
# ============================================================
base_out_dir = os.path.expanduser("~/Scratch/vlm_prompt_dataset")

def load_split_csv(path, base_out_dir):
    df = pd.read_csv(path)

    # binarylab â†’ binarylabel
    if "binarylab" in df.columns and "binarylabel" not in df.columns:
        df = df.rename(columns={"binarylab": "binarylabel"})

    # severity / dataset ì •ê·œí™”
    df["severity_norm"] = df["severity"].astype(str).str.lower()
    df["dataset_norm"]  = df["dataset"].astype(str).str.lower()

    # strong ì œì™¸
    df = df[df["severity_norm"].isin(["clean", "weak"])].copy()

    # Windows path â†’ Myriad path ë³€í™˜
    df["filepath"] = (
        df["filepath"]
        .astype(str)
        .str.replace(
            r"C:\Users\hanna\Lectures\Research_Project\Codes\Dataset\vlm_prompt_dataset",
            base_out_dir,
            regex=False,
        )
        .str.replace("\\", "/", regex=False)
    )

    if "fileindex" not in df.columns:
        raise ValueError(f"{path} ì— fileindex ì»¬ëŸ¼ í•„ìš”í•¨")

    return df

train_df = load_split_csv(TRAIN_CSV, base_out_dir)
val_df   = load_split_csv(VAL_CSV, base_out_dir)


class CleanWeakPairDataset(Dataset):
    """
    fileindex ê¸°ì¤€ìœ¼ë¡œ clean / weak í˜ì–´ë¥¼ ê°€ì ¸ì˜¤ëŠ” Dataset.
    """
    def __init__(self, df, processor, prompt_by_dataset, system_prompt=None):
        self.df = df.copy()
        self.processor = processor
        self.prompt_by_dataset = prompt_by_dataset
        self.system_prompt = system_prompt  # ì§€ê¸ˆì€ ì‚¬ìš© ì•ˆ í•¨

        self.df["severity_norm"] = self.df["severity"].astype(str).str.lower()
        self.df["dataset_norm"]  = self.df["dataset"].astype(str).str.lower()

        self.pairs = []  # (clean_row, weak_row)

        for fid, g in self.df.groupby("fileindex"):
            g = g.reset_index(drop=True)
            clean_rows = g[g["severity_norm"] == "clean"]
            weak_rows  = g[g["severity_norm"] == "weak"]

            if len(clean_rows) == 0 or len(weak_rows) == 0:
                continue

            # cleanì€ í•œ ì¥(anchor)ë§Œ ì“°ê³ 
            clean_row = clean_rows.iloc[0]

            # weak ê°œìˆ˜ë§Œí¼ pair ìƒì„±
            for _, weak_row in weak_rows.iterrows():
                self.pairs.append((clean_row, weak_row))
       
        if len(self.pairs) == 0:
            raise RuntimeError("clean/weak í˜ì–´ ì—†ìŒ. fileindex/severity í™•ì¸ í•„ìš”.")
        print(f"Dataset - found {len(self.pairs)} cleanâ€“weak pairs.")

    def _make_sample(self, row):
        img_path = row["filepath"]
        if not os.path.exists(img_path):
            raise FileNotFoundError(img_path)

        img = Image.open(img_path).convert("RGB")
        modality = row["dataset_norm"]
        label = int(row["binarylabel"])

        prompt = self.prompt_by_dataset.get(
            modality,
            "This is a medical image.\nQuestion: Does this image show normal anatomy or signs of disease?\n\n",
        )

        # system prompt ì œê±°, ê·¸ëŒ€ë¡œ ì‚¬ìš©
        full_text = prompt

        label_text = "normal" if label == 0 else "disease"

        return {
            "image": img,
            "input_text": full_text,
            "label_text": label_text,
            "label": label,
        }

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        clean_row, weak_row = self.pairs[idx]
        clean_sample = self._make_sample(clean_row)
        weak_sample  = self._make_sample(weak_row)
        return {
            "clean": clean_sample,
            "weak":  weak_sample,
        }


def make_clean_weak_collate_fn(processor, backend):
    def collate(batch):
        def build_inputs(which):
            images, texts, labels_int = [], [], []
            for item in batch:
                s = item[which]
                images.append(s["image"])
                texts.append(s["input_text"])
                labels_int.append(s["label"])

            # Lingshu(Qwen2.5-VL) ì „ìš©
            if backend == "lingshu":
                messages_list = []
                for img, txt in zip(images, texts):
                    messages_list.append(
                        [
                            {
                                "role": "user",
                                "content": [
                                    {"type": "image", "image": img},
                                    {"type": "text", "text": txt},
                                ],
                            }
                        ]
                    )

                chat_texts = [
                    processor.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=False,
                    )
                    for messages in messages_list
                ]

                model_inputs = processor(
                    text=chat_texts,
                    images=images,
                    padding=True,
                    return_tensors="pt",
                )

            # InternVL ì „ìš©
            elif backend == "internvl":
                image_token = getattr(processor, "image_token", "<image>")
                inline_texts = [f"{image_token}\n{txt}" for txt in texts]

                model_inputs = processor(
                    text=inline_texts,
                    images=images,
                    return_tensors="pt",
                    padding=True,
                )

            # ê¸°íƒ€ backend (ë‚˜ì¤‘ í™•ì¥ìš©)
            else:
                model_inputs = processor(
                    text=texts,
                    images=images,
                    return_tensors="pt",
                    padding=True,
                )

            labels_cls = torch.tensor(labels_int, dtype=torch.long)

            out = dict(model_inputs)
            out["labels_cls"] = labels_cls
            return out

        return {
            "clean": build_inputs("clean"),
            "weak":  build_inputs("weak"),
        }

    return collate


# ============================================================
# 4. ê³µí†µ helper (train/val í•œ epoch)
# ============================================================
def to_device(batch_dict):
    out = {}
    for k, v in batch_dict.items():
        if isinstance(v, torch.Tensor):
            out[k] = v.to(device)
        else:
            out[k] = v
    return out


def last_hidden(sp_model: SoftPromptVLM, outputs):
    """
    SoftPromptVLM.extract_features ì‚¬ìš©í•´ì„œ (B, H) feature ì¶”ì¶œ
    """
    return sp_model.extract_features(outputs)


def run_epoch(sp_model, loader, optimizer=None):
    train = optimizer is not None
    if train:
        sp_model.train()
    else:
        sp_model.eval()

    total_loss = 0.0
    total_ce   = 0.0
    total_cons = 0.0
    n_steps = 0

    # ---- accuracy ê³„ì‚°ìš© ----
    correct = 0
    total   = 0

    with torch.set_grad_enabled(train):
        for batch in loader:
            clean = to_device(batch["clean"])
            weak  = to_device(batch["weak"])

            labels_cls_clean = clean["labels_cls"]
            labels_cls_weak  = weak["labels_cls"]

            def forward_one(mode_dict):
                mode_dict = dict(mode_dict)
                mode_dict.pop("labels_token", None)
                mode_dict.pop("labels_cls", None)
                outputs = sp_model(**mode_dict)
                return outputs

            out_c = forward_one(clean)
            out_w = forward_one(weak)

            # 1) representation ì¶”ì¶œ
            h_c = last_hidden(sp_model, out_c)  # (B, H)
            h_w = last_hidden(sp_model, out_w)  # (B, H)

            # 2) classifierë¥¼ í†µí•œ CE loss
            logits_c = sp_model.classifier(h_c)  # (B, 2)
            logits_w = sp_model.classifier(h_w)  # (B, 2)

            # ---- accuracy update ----
            with torch.no_grad():
                preds_c = torch.argmax(logits_c, dim=1)
                preds_w = torch.argmax(logits_w, dim=1)

                correct += (preds_c == labels_cls_clean).sum().item()
                correct += (preds_w == labels_cls_weak).sum().item()

                total   += labels_cls_clean.numel()
                total   += labels_cls_weak.numel()

            # ---- loss ê³„ì‚° ----
            L_c = F.cross_entropy(logits_c, labels_cls_clean)
            L_w = F.cross_entropy(logits_w, labels_cls_weak)
            L_ce = w_c * L_c + w_w * L_w

            L_cons = lambda_w * F.mse_loss(h_c, h_w)
            loss = L_ce + L_cons

            if train:
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

            total_loss += loss.item()
            total_ce   += L_ce.item()
            total_cons += L_cons.item()
            n_steps += 1

    # ---- ì—¬ê¸°ë¶€í„° return ë¶€ë¶„ ìˆ˜ì • ----
    if n_steps == 0:
        return 0.0, 0.0, 0.0, 0.0

    acc = correct / total if total > 0 else 0.0
    return (
        total_loss / n_steps,
        total_ce   / n_steps,
        total_cons / n_steps,
        acc,
    )



# ============================================================
# 5. BACKENDë³„ë¡œ ìˆœì°¨ í•™ìŠµ (ì—¬ê¸°ì„œëŠ” internvlë§Œ)
# ============================================================
SAVE_DIR = "./soft_prompt_ckpt"
os.makedirs(SAVE_DIR, exist_ok=True)

for BACKEND in BACKENDS:
    print("\n==============================")
    print(f"ğŸš€ Training backend: {BACKEND}")
    print("==============================")

    model_id = MODEL_ID_BY_BACKEND[BACKEND]

    # 1) base_model, processor ë¡œë“œ
    base_model, processor = load_backend(BACKEND, model_id)

    # 2) soft-prompt ë˜í¼ & optimizer
    keep_input_ids = (BACKEND == "lingshu")
    sp_model = SoftPromptVLM(
        base_model,
        num_virtual_tokens=NUM_VIRTUAL_TOKENS,
        keep_input_ids=keep_input_ids,
    )

    
    # 3) ckpt ê²½ë¡œ ì„¤ì • (backendë³„ë¡œ ë‹¤ë¥´ê²Œ)
    ckpt_path = os.path.join(
        SAVE_DIR,
        f"{BACKEND}_soft_prompt_tuning_with_system_900.pt"
    )

    resume = False

    # 4) ì²´í¬í¬ì¸íŠ¸ ìˆìœ¼ë©´ soft_prompt + classifier ë¡œë“œí•´ì„œ ì´ì–´ í•™ìŠµ
    if os.path.exists(ckpt_path):
        print(f"ğŸ”„ Found checkpoint for {BACKEND} â†’ {ckpt_path}")
        ckpt = torch.load(ckpt_path, map_location=device)

        # num_virtual_tokensì´ ckptì— ì €ì¥ë¼ ìˆìœ¼ë©´ ë§ì¶°ì£¼ê¸° (ì˜µì…˜)
        n_ckpt = ckpt.get("num_virtual_tokens", NUM_VIRTUAL_TOKENS)
        if n_ckpt != sp_model.num_virtual_tokens:
            print(f"âš  num_virtual_tokens mismatch: ckpt={n_ckpt}, current={sp_model.num_virtual_tokens}")
            print("   â†’ ìƒˆë¡œ SoftPromptVLMì„ ckpt ì„¤ì •ì— ë§ì¶° ìƒì„±í•©ë‹ˆë‹¤.")
            sp_model = SoftPromptVLM(
                base_model,
                num_virtual_tokens=n_ckpt,
                keep_input_ids=keep_input_ids,
            )

        sp_model.soft_prompt.load_state_dict(ckpt["soft_prompt"])
        sp_model.classifier.load_state_dict(ckpt["classifier"])

        resume = True
        print(f"âœ… Loaded soft_prompt + classifier for {BACKEND}")
    else:
        print(f"âœ¨ No checkpoint for {BACKEND} â€” training from scratch")

    optimizer = AdamW(
        list(sp_model.soft_prompt.parameters()) +
        list(sp_model.classifier.parameters()),
        lr=LR,
    )

    # 3) Dataset / DataLoader
    train_ds = CleanWeakPairDataset(
        df=train_df,
        processor=processor,
        prompt_by_dataset=PROMPT_BY_DATASET,
        system_prompt=None,
    )
    val_ds = CleanWeakPairDataset(
        df=val_df,
        processor=processor,
        prompt_by_dataset=PROMPT_BY_DATASET,
        system_prompt=None,
    )

    collate_fn = make_clean_weak_collate_fn(processor, BACKEND)

    bs = BATCH_SIZE_BY_BACKEND.get(BACKEND, BATCH_SIZE_DEFAULT)
    print(f"[{BACKEND}] Using batch_size = {bs}")

    train_loader = DataLoader(
        train_ds,
        batch_size=bs,
        shuffle=True,
        collate_fn=collate_fn,
    )
    val_loader = DataLoader(
        val_ds,
        batch_size=bs,
        shuffle=False,
        collate_fn=collate_fn,
    )

    print(f"[{BACKEND}] #train_pairs = {len(train_ds)}, #val_pairs = {len(val_ds)}")


    # 4) í•™ìŠµ ë£¨í”„
    for epoch in range(EPOCHS):
        tr_loss, tr_ce, tr_cons, tr_acc = run_epoch(sp_model, train_loader, optimizer)
        val_loss, val_ce, val_cons, val_acc = run_epoch(sp_model, val_loader, optimizer=None)
        tag = "RESUME" if resume else "FRESH"
        print(
            f"[{BACKEND}][{tag}] Epoch {epoch+1}/{EPOCHS} | "
            f"train: total={tr_loss:.4f}, CE={tr_ce:.4f}, Cons={tr_cons:.4f}, Acc={tr_acc*100:.2f}% | "
            f"val: total={val_loss:.4f}, CE={val_ce:.4f}, Cons={val_cons:.4f}, Acc={val_acc*100:.2f}%"
        )



    # 8) soft_prompt + classifier ì €ì¥ (internvl / lingshu ë‘˜ ë‹¤ ê³µí†µ)
    torch.save(
        {
            "soft_prompt": sp_model.soft_prompt.state_dict(),
            "classifier": sp_model.classifier.state_dict(),
            "num_virtual_tokens": sp_model.num_virtual_tokens,
            "backend": BACKEND,
            "model_id": model_id,
        },
        ckpt_path,
    )
    print(f"âœ… Saved soft_prompt for {BACKEND} â†’ {ckpt_path}")

    del sp_model, base_model, processor, optimizer, train_ds, val_ds, train_loader, val_loader
    torch.cuda.empty_cache()
