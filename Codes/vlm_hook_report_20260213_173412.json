{
  "run_id": "20260213_173412",
  "device": "cuda",
  "reports": [
    {
      "backend": "qwen3",
      "model_id": "Qwen/Qwen3-VL-8B-Instruct",
      "device": "cuda",
      "dtype_first_param": "float16",
      "tree_depth": 3,
      "tree_top": [
        [
          "model",
          "Qwen3VLModel"
        ],
        [
          "model.visual",
          "Qwen3VLVisionModel"
        ],
        [
          "model.visual.patch_embed",
          "Qwen3VLVisionPatchEmbed"
        ],
        [
          "model.visual.patch_embed.proj",
          "Conv3d"
        ],
        [
          "model.visual.pos_embed",
          "Embedding"
        ],
        [
          "model.visual.rotary_pos_emb",
          "Qwen3VLVisionRotaryEmbedding"
        ],
        [
          "model.visual.blocks",
          "ModuleList"
        ],
        [
          "model.visual.blocks.0",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.1",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.2",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.3",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.4",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.5",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.6",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.7",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.8",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.9",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.10",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.11",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.12",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.13",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.14",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.15",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.16",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.17",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.18",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.19",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.20",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.21",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.22",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.23",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.24",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.25",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.blocks.26",
          "Qwen3VLVisionBlock"
        ],
        [
          "model.visual.merger",
          "Qwen3VLVisionPatchMerger"
        ],
        [
          "model.visual.merger.norm",
          "LayerNorm"
        ],
        [
          "model.visual.merger.linear_fc1",
          "Linear"
        ],
        [
          "model.visual.merger.act_fn",
          "GELU"
        ],
        [
          "model.visual.merger.linear_fc2",
          "Linear"
        ],
        [
          "model.visual.deepstack_merger_list",
          "ModuleList"
        ],
        [
          "model.visual.deepstack_merger_list.0",
          "Qwen3VLVisionPatchMerger"
        ],
        [
          "model.visual.deepstack_merger_list.1",
          "Qwen3VLVisionPatchMerger"
        ],
        [
          "model.visual.deepstack_merger_list.2",
          "Qwen3VLVisionPatchMerger"
        ],
        [
          "model.language_model",
          "Qwen3VLTextModel"
        ],
        [
          "model.language_model.embed_tokens",
          "Embedding"
        ],
        [
          "model.language_model.layers",
          "ModuleList"
        ],
        [
          "model.language_model.layers.0",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.1",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.2",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.3",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.4",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.5",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.6",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.7",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.8",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.9",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.10",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.11",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.12",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.13",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.14",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.15",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.16",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.17",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.18",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.19",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.20",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.21",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.22",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.23",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.24",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.25",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.26",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.27",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.28",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.29",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.30",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.31",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.32",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.33",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.34",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.layers.35",
          "Qwen3VLTextDecoderLayer"
        ],
        [
          "model.language_model.norm",
          "Qwen3VLTextRMSNorm"
        ],
        [
          "model.language_model.rotary_emb",
          "Qwen3VLTextRotaryEmbedding"
        ],
        [
          "lm_head",
          "Linear"
        ]
      ],
      "module_hits": {
        "vision_like": [
          [
            "model.visual",
            "Qwen3VLVisionModel"
          ],
          [
            "model.visual.patch_embed",
            "Qwen3VLVisionPatchEmbed"
          ],
          [
            "model.visual.patch_embed.proj",
            "Conv3d"
          ],
          [
            "model.visual.pos_embed",
            "Embedding"
          ],
          [
            "model.visual.rotary_pos_emb",
            "Qwen3VLVisionRotaryEmbedding"
          ],
          [
            "model.visual.blocks",
            "ModuleList"
          ],
          [
            "model.visual.blocks.0",
            "Qwen3VLVisionBlock"
          ],
          [
            "model.visual.blocks.0.norm1",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.0.norm2",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.0.attn",
            "Qwen3VLVisionAttention"
          ],
          [
            "model.visual.blocks.0.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.0.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.0.mlp",
            "Qwen3VLVisionMLP"
          ],
          [
            "model.visual.blocks.0.mlp.linear_fc1",
            "Linear"
          ],
          [
            "model.visual.blocks.0.mlp.linear_fc2",
            "Linear"
          ],
          [
            "model.visual.blocks.0.mlp.act_fn",
            "GELUTanh"
          ],
          [
            "model.visual.blocks.1",
            "Qwen3VLVisionBlock"
          ],
          [
            "model.visual.blocks.1.norm1",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.1.norm2",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.1.attn",
            "Qwen3VLVisionAttention"
          ],
          [
            "model.visual.blocks.1.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.1.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.1.mlp",
            "Qwen3VLVisionMLP"
          ],
          [
            "model.visual.blocks.1.mlp.linear_fc1",
            "Linear"
          ],
          [
            "model.visual.blocks.1.mlp.linear_fc2",
            "Linear"
          ],
          [
            "model.visual.blocks.1.mlp.act_fn",
            "GELUTanh"
          ],
          [
            "model.visual.blocks.2",
            "Qwen3VLVisionBlock"
          ],
          [
            "model.visual.blocks.2.norm1",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.2.norm2",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.2.attn",
            "Qwen3VLVisionAttention"
          ],
          [
            "model.visual.blocks.2.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.2.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.2.mlp",
            "Qwen3VLVisionMLP"
          ],
          [
            "model.visual.blocks.2.mlp.linear_fc1",
            "Linear"
          ],
          [
            "model.visual.blocks.2.mlp.linear_fc2",
            "Linear"
          ],
          [
            "model.visual.blocks.2.mlp.act_fn",
            "GELUTanh"
          ],
          [
            "model.visual.blocks.3",
            "Qwen3VLVisionBlock"
          ],
          [
            "model.visual.blocks.3.norm1",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.3.norm2",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.3.attn",
            "Qwen3VLVisionAttention"
          ],
          [
            "model.visual.blocks.3.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.3.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.3.mlp",
            "Qwen3VLVisionMLP"
          ],
          [
            "model.visual.blocks.3.mlp.linear_fc1",
            "Linear"
          ],
          [
            "model.visual.blocks.3.mlp.linear_fc2",
            "Linear"
          ],
          [
            "model.visual.blocks.3.mlp.act_fn",
            "GELUTanh"
          ],
          [
            "model.visual.blocks.4",
            "Qwen3VLVisionBlock"
          ],
          [
            "model.visual.blocks.4.norm1",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.4.norm2",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.4.attn",
            "Qwen3VLVisionAttention"
          ],
          [
            "model.visual.blocks.4.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.4.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.4.mlp",
            "Qwen3VLVisionMLP"
          ],
          [
            "model.visual.blocks.4.mlp.linear_fc1",
            "Linear"
          ],
          [
            "model.visual.blocks.4.mlp.linear_fc2",
            "Linear"
          ],
          [
            "model.visual.blocks.4.mlp.act_fn",
            "GELUTanh"
          ],
          [
            "model.visual.blocks.5",
            "Qwen3VLVisionBlock"
          ],
          [
            "model.visual.blocks.5.norm1",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.5.norm2",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.5.attn",
            "Qwen3VLVisionAttention"
          ],
          [
            "model.visual.blocks.5.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.5.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.5.mlp",
            "Qwen3VLVisionMLP"
          ],
          [
            "model.visual.blocks.5.mlp.linear_fc1",
            "Linear"
          ],
          [
            "model.visual.blocks.5.mlp.linear_fc2",
            "Linear"
          ],
          [
            "model.visual.blocks.5.mlp.act_fn",
            "GELUTanh"
          ],
          [
            "model.visual.blocks.6",
            "Qwen3VLVisionBlock"
          ],
          [
            "model.visual.blocks.6.norm1",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.6.norm2",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.6.attn",
            "Qwen3VLVisionAttention"
          ],
          [
            "model.visual.blocks.6.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.6.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.6.mlp",
            "Qwen3VLVisionMLP"
          ],
          [
            "model.visual.blocks.6.mlp.linear_fc1",
            "Linear"
          ],
          [
            "model.visual.blocks.6.mlp.linear_fc2",
            "Linear"
          ],
          [
            "model.visual.blocks.6.mlp.act_fn",
            "GELUTanh"
          ],
          [
            "model.visual.blocks.7",
            "Qwen3VLVisionBlock"
          ],
          [
            "model.visual.blocks.7.norm1",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.7.norm2",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.7.attn",
            "Qwen3VLVisionAttention"
          ],
          [
            "model.visual.blocks.7.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.7.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.7.mlp",
            "Qwen3VLVisionMLP"
          ],
          [
            "model.visual.blocks.7.mlp.linear_fc1",
            "Linear"
          ],
          [
            "model.visual.blocks.7.mlp.linear_fc2",
            "Linear"
          ],
          [
            "model.visual.blocks.7.mlp.act_fn",
            "GELUTanh"
          ],
          [
            "model.visual.blocks.8",
            "Qwen3VLVisionBlock"
          ],
          [
            "model.visual.blocks.8.norm1",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.8.norm2",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.8.attn",
            "Qwen3VLVisionAttention"
          ],
          [
            "model.visual.blocks.8.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.8.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.8.mlp",
            "Qwen3VLVisionMLP"
          ],
          [
            "model.visual.blocks.8.mlp.linear_fc1",
            "Linear"
          ],
          [
            "model.visual.blocks.8.mlp.linear_fc2",
            "Linear"
          ],
          [
            "model.visual.blocks.8.mlp.act_fn",
            "GELUTanh"
          ],
          [
            "model.visual.blocks.9",
            "Qwen3VLVisionBlock"
          ],
          [
            "model.visual.blocks.9.norm1",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.9.norm2",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.9.attn",
            "Qwen3VLVisionAttention"
          ],
          [
            "model.visual.blocks.9.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.9.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.9.mlp",
            "Qwen3VLVisionMLP"
          ],
          [
            "model.visual.blocks.9.mlp.linear_fc1",
            "Linear"
          ],
          [
            "model.visual.blocks.9.mlp.linear_fc2",
            "Linear"
          ],
          [
            "model.visual.blocks.9.mlp.act_fn",
            "GELUTanh"
          ],
          [
            "model.visual.blocks.10",
            "Qwen3VLVisionBlock"
          ],
          [
            "model.visual.blocks.10.norm1",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.10.norm2",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.10.attn",
            "Qwen3VLVisionAttention"
          ],
          [
            "model.visual.blocks.10.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.10.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.10.mlp",
            "Qwen3VLVisionMLP"
          ],
          [
            "model.visual.blocks.10.mlp.linear_fc1",
            "Linear"
          ],
          [
            "model.visual.blocks.10.mlp.linear_fc2",
            "Linear"
          ],
          [
            "model.visual.blocks.10.mlp.act_fn",
            "GELUTanh"
          ],
          [
            "model.visual.blocks.11",
            "Qwen3VLVisionBlock"
          ],
          [
            "model.visual.blocks.11.norm1",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.11.norm2",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.11.attn",
            "Qwen3VLVisionAttention"
          ],
          [
            "model.visual.blocks.11.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.11.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.11.mlp",
            "Qwen3VLVisionMLP"
          ],
          [
            "model.visual.blocks.11.mlp.linear_fc1",
            "Linear"
          ],
          [
            "model.visual.blocks.11.mlp.linear_fc2",
            "Linear"
          ],
          [
            "model.visual.blocks.11.mlp.act_fn",
            "GELUTanh"
          ],
          [
            "model.visual.blocks.12",
            "Qwen3VLVisionBlock"
          ],
          [
            "model.visual.blocks.12.norm1",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.12.norm2",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.12.attn",
            "Qwen3VLVisionAttention"
          ],
          [
            "model.visual.blocks.12.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.12.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.12.mlp",
            "Qwen3VLVisionMLP"
          ],
          [
            "model.visual.blocks.12.mlp.linear_fc1",
            "Linear"
          ],
          [
            "model.visual.blocks.12.mlp.linear_fc2",
            "Linear"
          ],
          [
            "model.visual.blocks.12.mlp.act_fn",
            "GELUTanh"
          ],
          [
            "model.visual.blocks.13",
            "Qwen3VLVisionBlock"
          ],
          [
            "model.visual.blocks.13.norm1",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.13.norm2",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.13.attn",
            "Qwen3VLVisionAttention"
          ],
          [
            "model.visual.blocks.13.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.13.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.13.mlp",
            "Qwen3VLVisionMLP"
          ],
          [
            "model.visual.blocks.13.mlp.linear_fc1",
            "Linear"
          ],
          [
            "model.visual.blocks.13.mlp.linear_fc2",
            "Linear"
          ],
          [
            "model.visual.blocks.13.mlp.act_fn",
            "GELUTanh"
          ],
          [
            "model.visual.blocks.14",
            "Qwen3VLVisionBlock"
          ],
          [
            "model.visual.blocks.14.norm1",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.14.norm2",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.14.attn",
            "Qwen3VLVisionAttention"
          ],
          [
            "model.visual.blocks.14.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.14.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.14.mlp",
            "Qwen3VLVisionMLP"
          ],
          [
            "model.visual.blocks.14.mlp.linear_fc1",
            "Linear"
          ],
          [
            "model.visual.blocks.14.mlp.linear_fc2",
            "Linear"
          ],
          [
            "model.visual.blocks.14.mlp.act_fn",
            "GELUTanh"
          ],
          [
            "model.visual.blocks.15",
            "Qwen3VLVisionBlock"
          ],
          [
            "model.visual.blocks.15.norm1",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.15.norm2",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.15.attn",
            "Qwen3VLVisionAttention"
          ],
          [
            "model.visual.blocks.15.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.15.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.15.mlp",
            "Qwen3VLVisionMLP"
          ],
          [
            "model.visual.blocks.15.mlp.linear_fc1",
            "Linear"
          ],
          [
            "model.visual.blocks.15.mlp.linear_fc2",
            "Linear"
          ],
          [
            "model.visual.blocks.15.mlp.act_fn",
            "GELUTanh"
          ],
          [
            "model.visual.blocks.16",
            "Qwen3VLVisionBlock"
          ],
          [
            "model.visual.blocks.16.norm1",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.16.norm2",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.16.attn",
            "Qwen3VLVisionAttention"
          ],
          [
            "model.visual.blocks.16.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.16.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.16.mlp",
            "Qwen3VLVisionMLP"
          ],
          [
            "model.visual.blocks.16.mlp.linear_fc1",
            "Linear"
          ],
          [
            "model.visual.blocks.16.mlp.linear_fc2",
            "Linear"
          ],
          [
            "model.visual.blocks.16.mlp.act_fn",
            "GELUTanh"
          ],
          [
            "model.visual.blocks.17",
            "Qwen3VLVisionBlock"
          ],
          [
            "model.visual.blocks.17.norm1",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.17.norm2",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.17.attn",
            "Qwen3VLVisionAttention"
          ],
          [
            "model.visual.blocks.17.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.17.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.17.mlp",
            "Qwen3VLVisionMLP"
          ],
          [
            "model.visual.blocks.17.mlp.linear_fc1",
            "Linear"
          ],
          [
            "model.visual.blocks.17.mlp.linear_fc2",
            "Linear"
          ],
          [
            "model.visual.blocks.17.mlp.act_fn",
            "GELUTanh"
          ],
          [
            "model.visual.blocks.18",
            "Qwen3VLVisionBlock"
          ],
          [
            "model.visual.blocks.18.norm1",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.18.norm2",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.18.attn",
            "Qwen3VLVisionAttention"
          ],
          [
            "model.visual.blocks.18.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.18.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.18.mlp",
            "Qwen3VLVisionMLP"
          ],
          [
            "model.visual.blocks.18.mlp.linear_fc1",
            "Linear"
          ],
          [
            "model.visual.blocks.18.mlp.linear_fc2",
            "Linear"
          ],
          [
            "model.visual.blocks.18.mlp.act_fn",
            "GELUTanh"
          ],
          [
            "model.visual.blocks.19",
            "Qwen3VLVisionBlock"
          ],
          [
            "model.visual.blocks.19.norm1",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.19.norm2",
            "LayerNorm"
          ],
          [
            "model.visual.blocks.19.attn",
            "Qwen3VLVisionAttention"
          ]
        ],
        "projector_like": [],
        "fusion_like": [],
        "adapter_like": []
      },
      "hook_candidates_registered": {
        "vision": [
          "model.visual",
          "model.visual.patch_embed",
          "model.visual.patch_embed.proj",
          "model.visual.pos_embed",
          "model.visual.rotary_pos_emb",
          "model.visual.blocks",
          "model.visual.blocks.0",
          "model.visual.blocks.0.norm1",
          "model.visual.blocks.0.norm2",
          "model.visual.blocks.0.attn",
          "model.visual.blocks.0.attn.qkv",
          "model.visual.blocks.0.attn.proj",
          "model.visual.blocks.0.mlp",
          "model.visual.blocks.0.mlp.linear_fc1",
          "model.visual.blocks.0.mlp.linear_fc2",
          "model.visual.blocks.0.mlp.act_fn",
          "model.visual.blocks.1",
          "model.visual.blocks.1.norm1",
          "model.visual.blocks.1.norm2",
          "model.visual.blocks.1.attn",
          "model.visual.blocks.1.attn.qkv",
          "model.visual.blocks.1.attn.proj",
          "model.visual.blocks.1.mlp",
          "model.visual.blocks.1.mlp.linear_fc1",
          "model.visual.blocks.1.mlp.linear_fc2",
          "model.visual.blocks.1.mlp.act_fn",
          "model.visual.blocks.2",
          "model.visual.blocks.2.norm1",
          "model.visual.blocks.2.norm2",
          "model.visual.blocks.2.attn"
        ],
        "projector": [
          "model.visual.merger.linear_fc2",
          "model.visual.merger",
          "model.visual.deepstack_merger_list.0.linear_fc2",
          "model.visual.deepstack_merger_list.1.linear_fc2",
          "model.visual.deepstack_merger_list.2.linear_fc2",
          "model.visual"
        ],
        "adapter": [],
        "fusion": []
      },
      "hook_fired_summary": [
        {
          "name": "vision:model.visual.patch_embed.proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1152,
              1,
              1,
              1
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -2.98828125,
            "max": 4.39453125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.patch_embed",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1152
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -2.98828125,
            "max": 4.39453125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.pos_embed",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              4,
              256,
              1152
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -12.875,
            "max": 18.875,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.rotary_pos_emb",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              16,
              18
            ],
            "dtype": "float32",
            "device": "cuda:0",
            "min": 0.0,
            "max": 15.0,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.0.norm1",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1152
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -10.046875,
            "max": 9.1875,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.0.attn.qkv",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              3456
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -7.35546875,
            "max": 7.46875,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.0.attn.proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1152
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -6.5078125,
            "max": 4.65234375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.0.attn",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1152
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -6.5078125,
            "max": 4.65234375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.0.norm2",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1152
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -26.640625,
            "max": 24.65625,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.0.mlp.linear_fc1",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              4304
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -14.40625,
            "max": 15.2890625,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.0.mlp.act_fn",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              4304
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -0.1700439453125,
            "max": 15.2890625,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.0.mlp.linear_fc2",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1152
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -8.625,
            "max": 6.1640625,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.0.mlp",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1152
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -8.625,
            "max": 6.1640625,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.0",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1152
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -10.1171875,
            "max": 5.40625,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.1.norm1",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1152
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -7.11328125,
            "max": 11.578125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.1.attn.qkv",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              3456
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -10.6015625,
            "max": 9.7890625,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.1.attn.proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1152
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -5.0859375,
            "max": 4.375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.1.attn",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1152
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -5.0859375,
            "max": 4.375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.1.norm2",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1152
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -6.328125,
            "max": 12.8359375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.1.mlp.linear_fc1",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              4304
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -9.734375,
            "max": 4.4453125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.1.mlp.act_fn",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              4304
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -0.1700439453125,
            "max": 4.4453125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.1.mlp.linear_fc2",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1152
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -4.55859375,
            "max": 5.67578125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.1.mlp",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1152
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -4.55859375,
            "max": 5.67578125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.1",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1152
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -6.1328125,
            "max": 7.0703125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.2.norm1",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1152
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -10.609375,
            "max": 10.3984375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.2.attn",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1152
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -3.064453125,
            "max": 1.9228515625,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.2.norm2",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1152
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -5.70703125,
            "max": 13.8359375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.2",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1152
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -4.3828125,
            "max": 11.953125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "projector:model.visual.deepstack_merger_list.0.linear_fc2",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              64,
              4096
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -2.337890625,
            "max": 2.96875,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "projector:model.visual.deepstack_merger_list.1.linear_fc2",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              64,
              4096
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -2.27734375,
            "max": 1.982421875,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "projector:model.visual.deepstack_merger_list.2.linear_fc2",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              64,
              4096
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -1.107421875,
            "max": 1.4580078125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "projector:model.visual.merger.linear_fc2",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              64,
              4096
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -4.890625,
            "max": 4.26171875,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "projector:model.visual.merger",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              64,
              4096
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -4.890625,
            "max": 4.26171875,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual",
          "calls": 1,
          "first_output": {
            "type": "dict",
            "keys": [
              "last_hidden_state",
              "pooler_output",
              "deepstack_features"
            ],
            "tensors": {
              "last_hidden_state": {
                "shape": [
                  256,
                  1152
                ],
                "dtype": "float16",
                "device": "cuda:0",
                "min": -1713.0,
                "max": 13688.0,
                "nan": 0,
                "inf": 0
              },
              "pooler_output": {
                "shape": [
                  64,
                  4096
                ],
                "dtype": "float16",
                "device": "cuda:0",
                "min": -4.890625,
                "max": 4.26171875,
                "nan": 0,
                "inf": 0
              }
            }
          }
        },
        {
          "name": "projector:model.visual",
          "calls": 1,
          "first_output": {
            "type": "dict",
            "keys": [
              "last_hidden_state",
              "pooler_output",
              "deepstack_features"
            ],
            "tensors": {
              "last_hidden_state": {
                "shape": [
                  256,
                  1152
                ],
                "dtype": "float16",
                "device": "cuda:0",
                "min": -1713.0,
                "max": 13688.0,
                "nan": 0,
                "inf": 0
              },
              "pooler_output": {
                "shape": [
                  64,
                  4096
                ],
                "dtype": "float16",
                "device": "cuda:0",
                "min": -4.890625,
                "max": 4.26171875,
                "nan": 0,
                "inf": 0
              }
            }
          }
        }
      ],
      "hookable": {
        "vision_path_hookable": true,
        "projector_path_hookable": false
      },
      "forward_kwargs_tried": {
        "used_kwargs": {
          "output_hidden_states": true,
          "output_attentions": true,
          "return_dict": true
        }
      },
      "top_level_outputs": {
        "type": "Qwen3VLCausalLMOutputWithPast",
        "hidden_states": {
          "type": "tuple",
          "len": 37,
          "last": {
            "type": "tensor",
            "shape": [
              1,
              103,
              4096
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -150.625,
            "max": 116.4375,
            "nan": 0,
            "inf": 0
          }
        },
        "logits": {
          "type": "tensor",
          "shape": [
            1,
            103,
            151936
          ],
          "dtype": "float16",
          "device": "cuda:0",
          "min": -26.078125,
          "max": 41.1875,
          "nan": 0,
          "inf": 0
        }
      },
      "notes": {
        "vision_encoder_extractable": "LIKELY",
        "projector_extractable": "UNCERTAIN (no projector-related hooks fired; module may be named differently)"
      }
    },
    {
      "backend": "medgemma",
      "model_id": "google/medgemma-1.5-4b-it",
      "device": "cuda",
      "dtype_first_param": "bfloat16",
      "tree_depth": 3,
      "tree_top": [
        [
          "model",
          "Gemma3Model"
        ],
        [
          "model.vision_tower",
          "SiglipVisionModel"
        ],
        [
          "model.vision_tower.vision_model",
          "SiglipVisionTransformer"
        ],
        [
          "model.vision_tower.vision_model.embeddings",
          "SiglipVisionEmbeddings"
        ],
        [
          "model.vision_tower.vision_model.encoder",
          "SiglipEncoder"
        ],
        [
          "model.vision_tower.vision_model.post_layernorm",
          "LayerNorm"
        ],
        [
          "model.multi_modal_projector",
          "Gemma3MultiModalProjector"
        ],
        [
          "model.multi_modal_projector.mm_soft_emb_norm",
          "Gemma3RMSNorm"
        ],
        [
          "model.multi_modal_projector.avg_pool",
          "AvgPool2d"
        ],
        [
          "model.language_model",
          "Gemma3TextModel"
        ],
        [
          "model.language_model.embed_tokens",
          "Gemma3TextScaledWordEmbedding"
        ],
        [
          "model.language_model.layers",
          "ModuleList"
        ],
        [
          "model.language_model.layers.0",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.1",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.2",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.3",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.4",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.5",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.6",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.7",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.8",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.9",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.10",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.11",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.12",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.13",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.14",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.15",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.16",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.17",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.18",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.19",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.20",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.21",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.22",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.23",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.24",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.25",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.26",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.27",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.28",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.29",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.30",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.31",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.32",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.layers.33",
          "Gemma3DecoderLayer"
        ],
        [
          "model.language_model.norm",
          "Gemma3RMSNorm"
        ],
        [
          "model.language_model.rotary_emb",
          "Gemma3RotaryEmbedding"
        ],
        [
          "lm_head",
          "Linear"
        ]
      ],
      "module_hits": {
        "vision_like": [
          [
            "model.vision_tower",
            "SiglipVisionModel"
          ],
          [
            "model.vision_tower.vision_model",
            "SiglipVisionTransformer"
          ],
          [
            "model.vision_tower.vision_model.embeddings",
            "SiglipVisionEmbeddings"
          ],
          [
            "model.vision_tower.vision_model.embeddings.patch_embedding",
            "Conv2d"
          ],
          [
            "model.vision_tower.vision_model.embeddings.position_embedding",
            "Embedding"
          ],
          [
            "model.vision_tower.vision_model.encoder",
            "SiglipEncoder"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers",
            "ModuleList"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.0",
            "SiglipEncoderLayer"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.0.layer_norm1",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.0.self_attn",
            "SiglipAttention"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.0.layer_norm2",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.0.mlp",
            "SiglipMLP"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.0.mlp.activation_fn",
            "GELUTanh"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.0.mlp.fc1",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.0.mlp.fc2",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.1",
            "SiglipEncoderLayer"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.1.layer_norm1",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.1.self_attn",
            "SiglipAttention"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.1.layer_norm2",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.1.mlp",
            "SiglipMLP"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.1.mlp.activation_fn",
            "GELUTanh"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.1.mlp.fc1",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.1.mlp.fc2",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.2",
            "SiglipEncoderLayer"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.2.layer_norm1",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.2.self_attn",
            "SiglipAttention"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.2.layer_norm2",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.2.mlp",
            "SiglipMLP"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.2.mlp.activation_fn",
            "GELUTanh"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.2.mlp.fc1",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.2.mlp.fc2",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.3",
            "SiglipEncoderLayer"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.3.layer_norm1",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.3.self_attn",
            "SiglipAttention"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.3.layer_norm2",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.3.mlp",
            "SiglipMLP"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.3.mlp.activation_fn",
            "GELUTanh"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.3.mlp.fc1",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.3.mlp.fc2",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.4",
            "SiglipEncoderLayer"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.4.layer_norm1",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.4.self_attn",
            "SiglipAttention"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.4.layer_norm2",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.4.mlp",
            "SiglipMLP"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.4.mlp.activation_fn",
            "GELUTanh"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.4.mlp.fc1",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.4.mlp.fc2",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.5",
            "SiglipEncoderLayer"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.5.layer_norm1",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.5.self_attn",
            "SiglipAttention"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.5.layer_norm2",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.5.mlp",
            "SiglipMLP"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.5.mlp.activation_fn",
            "GELUTanh"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.5.mlp.fc1",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.5.mlp.fc2",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.6",
            "SiglipEncoderLayer"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.6.layer_norm1",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.6.self_attn",
            "SiglipAttention"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.6.layer_norm2",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.6.mlp",
            "SiglipMLP"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.6.mlp.activation_fn",
            "GELUTanh"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.6.mlp.fc1",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.6.mlp.fc2",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.7",
            "SiglipEncoderLayer"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.7.layer_norm1",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.7.self_attn",
            "SiglipAttention"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.7.layer_norm2",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.7.mlp",
            "SiglipMLP"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.7.mlp.activation_fn",
            "GELUTanh"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.7.mlp.fc1",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.7.mlp.fc2",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.8",
            "SiglipEncoderLayer"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.8.layer_norm1",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.8.self_attn",
            "SiglipAttention"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.8.layer_norm2",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.8.mlp",
            "SiglipMLP"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.8.mlp.activation_fn",
            "GELUTanh"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.8.mlp.fc1",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.8.mlp.fc2",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.9",
            "SiglipEncoderLayer"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.9.layer_norm1",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.9.self_attn",
            "SiglipAttention"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.9.layer_norm2",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.9.mlp",
            "SiglipMLP"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.9.mlp.activation_fn",
            "GELUTanh"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.9.mlp.fc1",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.9.mlp.fc2",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.10",
            "SiglipEncoderLayer"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.10.layer_norm1",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.10.self_attn",
            "SiglipAttention"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.10.layer_norm2",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.10.mlp",
            "SiglipMLP"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.10.mlp.activation_fn",
            "GELUTanh"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.10.mlp.fc1",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.10.mlp.fc2",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.11",
            "SiglipEncoderLayer"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.11.layer_norm1",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.11.self_attn",
            "SiglipAttention"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.11.layer_norm2",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.11.mlp",
            "SiglipMLP"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.11.mlp.activation_fn",
            "GELUTanh"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.11.mlp.fc1",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.11.mlp.fc2",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.12",
            "SiglipEncoderLayer"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.12.layer_norm1",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.12.self_attn",
            "SiglipAttention"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.12.layer_norm2",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.12.mlp",
            "SiglipMLP"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.12.mlp.activation_fn",
            "GELUTanh"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.12.mlp.fc1",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.12.mlp.fc2",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.13",
            "SiglipEncoderLayer"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.13.layer_norm1",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.13.self_attn",
            "SiglipAttention"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.13.layer_norm2",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.13.mlp",
            "SiglipMLP"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.13.mlp.activation_fn",
            "GELUTanh"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.13.mlp.fc1",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.13.mlp.fc2",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.14",
            "SiglipEncoderLayer"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.14.layer_norm1",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.14.self_attn",
            "SiglipAttention"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.14.layer_norm2",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.14.mlp",
            "SiglipMLP"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.14.mlp.activation_fn",
            "GELUTanh"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.14.mlp.fc1",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.14.mlp.fc2",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.15",
            "SiglipEncoderLayer"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.15.layer_norm1",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.15.self_attn",
            "SiglipAttention"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.15.layer_norm2",
            "LayerNorm"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.15.mlp",
            "SiglipMLP"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.15.mlp.activation_fn",
            "GELUTanh"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.15.mlp.fc1",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.15.mlp.fc2",
            "Linear"
          ],
          [
            "model.vision_tower.vision_model.encoder.layers.16",
            "SiglipEncoderLayer"
          ]
        ],
        "projector_like": [
          [
            "model.multi_modal_projector",
            "Gemma3MultiModalProjector"
          ],
          [
            "model.multi_modal_projector.mm_soft_emb_norm",
            "Gemma3RMSNorm"
          ],
          [
            "model.multi_modal_projector.avg_pool",
            "AvgPool2d"
          ]
        ],
        "fusion_like": [],
        "adapter_like": []
      },
      "hook_candidates_registered": {
        "vision": [
          "model.vision_tower",
          "model.vision_tower.vision_model",
          "model.vision_tower.vision_model.embeddings",
          "model.vision_tower.vision_model.embeddings.patch_embedding",
          "model.vision_tower.vision_model.embeddings.position_embedding",
          "model.vision_tower.vision_model.encoder",
          "model.vision_tower.vision_model.encoder.layers",
          "model.vision_tower.vision_model.encoder.layers.0",
          "model.vision_tower.vision_model.encoder.layers.0.layer_norm1",
          "model.vision_tower.vision_model.encoder.layers.0.self_attn",
          "model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj",
          "model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj",
          "model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj",
          "model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj",
          "model.vision_tower.vision_model.encoder.layers.0.layer_norm2",
          "model.vision_tower.vision_model.encoder.layers.0.mlp",
          "model.vision_tower.vision_model.encoder.layers.0.mlp.activation_fn",
          "model.vision_tower.vision_model.encoder.layers.0.mlp.fc1",
          "model.vision_tower.vision_model.encoder.layers.0.mlp.fc2",
          "model.vision_tower.vision_model.encoder.layers.1",
          "model.vision_tower.vision_model.encoder.layers.1.layer_norm1",
          "model.vision_tower.vision_model.encoder.layers.1.self_attn",
          "model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj",
          "model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj",
          "model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj",
          "model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj",
          "model.vision_tower.vision_model.encoder.layers.1.layer_norm2",
          "model.vision_tower.vision_model.encoder.layers.1.mlp",
          "model.vision_tower.vision_model.encoder.layers.1.mlp.activation_fn",
          "model.vision_tower.vision_model.encoder.layers.1.mlp.fc1"
        ],
        "projector": [
          "model.multi_modal_projector"
        ],
        "adapter": [],
        "fusion": [
          "model.multi_modal_projector.mm_soft_emb_norm"
        ]
      },
      "hook_fired_summary": [
        {
          "name": "vision:model.vision_tower.vision_model.embeddings.patch_embedding",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1152,
              64,
              64
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -15.4375,
            "max": 15.0625,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.embeddings.position_embedding",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              1152
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -76.0,
            "max": 234.0,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.embeddings",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              1152
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -80.5,
            "max": 233.0,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.0.layer_norm1",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              1152
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -4.125,
            "max": 3.3125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              1152
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -8.4375,
            "max": 8.5,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              1152
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -8.875,
            "max": 8.75,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              1152
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -7.3125,
            "max": 8.0625,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              1152
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -119.0,
            "max": 115.0,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.0.self_attn",
          "calls": 1,
          "first_output": {
            "type": "tuple",
            "len": 2,
            "items": [
              {
                "i": 0,
                "shape": [
                  1,
                  4096,
                  1152
                ],
                "dtype": "bfloat16",
                "device": "cuda:0",
                "min": -119.0,
                "max": 115.0,
                "nan": 0,
                "inf": 0
              },
              {
                "i": 1,
                "type": "NoneType"
              }
            ]
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.0.layer_norm2",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              1152
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -15.1875,
            "max": 8.125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.0.mlp.fc1",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              4304
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -15.4375,
            "max": 13.875,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.0.mlp.activation_fn",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              4304
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -0.169921875,
            "max": 13.875,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.0.mlp.fc2",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              1152
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -47.5,
            "max": 61.25,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.0.mlp",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              1152
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -47.5,
            "max": 61.25,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.0",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              1152
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -109.5,
            "max": 236.0,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.1.layer_norm1",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              1152
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -3.75,
            "max": 3.3125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              1152
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -7.28125,
            "max": 7.28125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              1152
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -6.6875,
            "max": 7.78125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              1152
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -4.875,
            "max": 4.65625,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              1152
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -14.9375,
            "max": 11.5,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.1.self_attn",
          "calls": 1,
          "first_output": {
            "type": "tuple",
            "len": 2,
            "items": [
              {
                "i": 0,
                "shape": [
                  1,
                  4096,
                  1152
                ],
                "dtype": "bfloat16",
                "device": "cuda:0",
                "min": -14.9375,
                "max": 11.5,
                "nan": 0,
                "inf": 0
              },
              {
                "i": 1,
                "type": "NoneType"
              }
            ]
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.1.layer_norm2",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              1152
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -4.78125,
            "max": 7.6875,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.1.mlp.fc1",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              4304
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -10.875,
            "max": 10.125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.1.mlp.activation_fn",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              4304
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -0.169921875,
            "max": 10.125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.1.mlp",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              1152
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -55.5,
            "max": 86.0,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder.layers.1",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              4096,
              1152
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -157.0,
            "max": 282.0,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model.encoder",
          "calls": 1,
          "first_output": {
            "type": "dict",
            "keys": [
              "last_hidden_state"
            ],
            "tensors": {
              "last_hidden_state": {
                "shape": [
                  1,
                  4096,
                  1152
                ],
                "dtype": "bfloat16",
                "device": "cuda:0",
                "min": -3056.0,
                "max": 4128.0,
                "nan": 0,
                "inf": 0
              }
            }
          }
        },
        {
          "name": "vision:model.vision_tower.vision_model",
          "calls": 1,
          "first_output": {
            "type": "dict",
            "keys": [
              "last_hidden_state"
            ],
            "tensors": {
              "last_hidden_state": {
                "shape": [
                  1,
                  4096,
                  1152
                ],
                "dtype": "bfloat16",
                "device": "cuda:0",
                "min": -9.75,
                "max": 101.0,
                "nan": 0,
                "inf": 0
              }
            }
          }
        },
        {
          "name": "vision:model.vision_tower",
          "calls": 1,
          "first_output": {
            "type": "dict",
            "keys": [
              "last_hidden_state"
            ],
            "tensors": {
              "last_hidden_state": {
                "shape": [
                  1,
                  4096,
                  1152
                ],
                "dtype": "bfloat16",
                "device": "cuda:0",
                "min": -9.75,
                "max": 101.0,
                "nan": 0,
                "inf": 0
              }
            }
          }
        },
        {
          "name": "fusion:model.multi_modal_projector.mm_soft_emb_norm",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              256,
              1152
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -2.890625,
            "max": 3.25,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "projector:model.multi_modal_projector",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              256,
              2560
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -3.46875,
            "max": 6.40625,
            "nan": 0,
            "inf": 0
          }
        }
      ],
      "hookable": {
        "vision_path_hookable": true,
        "projector_path_hookable": true
      },
      "forward_kwargs_tried": {
        "used_kwargs": {
          "output_hidden_states": true,
          "output_attentions": true,
          "return_dict": true
        }
      },
      "top_level_outputs": {
        "type": "Gemma3CausalLMOutputWithPast",
        "hidden_states": {
          "type": "tuple",
          "len": 35,
          "last": {
            "type": "tensor",
            "shape": [
              1,
              299,
              2560
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -79.0,
            "max": 65.5,
            "nan": 0,
            "inf": 0
          }
        },
        "logits": {
          "type": "tensor",
          "shape": [
            1,
            299,
            262208
          ],
          "dtype": "bfloat16",
          "device": "cuda:0",
          "min": -31.25,
          "max": 31.0,
          "nan": 0,
          "inf": 0
        }
      },
      "notes": {
        "vision_encoder_extractable": "LIKELY",
        "projector_extractable": "LIKELY"
      }
    },
    {
      "backend": "internvl",
      "model_id": "OpenGVLab/InternVL3_5-8B-HF",
      "device": "cuda",
      "dtype_first_param": "bfloat16",
      "tree_depth": 3,
      "tree_top": [
        [
          "vision_tower",
          "InternVLVisionModel"
        ],
        [
          "vision_tower.embeddings",
          "InternVLVisionEmbeddings"
        ],
        [
          "vision_tower.embeddings.patch_embeddings",
          "InternVLVisionPatchEmbeddings"
        ],
        [
          "vision_tower.embeddings.patch_embeddings.projection",
          "Conv2d"
        ],
        [
          "vision_tower.embeddings.dropout",
          "Dropout"
        ],
        [
          "vision_tower.encoder",
          "InternVLVisionEncoder"
        ],
        [
          "vision_tower.encoder.layer",
          "ModuleList"
        ],
        [
          "vision_tower.encoder.layer.0",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.1",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.2",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.3",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.4",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.5",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.6",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.7",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.8",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.9",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.10",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.11",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.12",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.13",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.14",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.15",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.16",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.17",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.18",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.19",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.20",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.21",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.22",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.encoder.layer.23",
          "InternVLVisionLayer"
        ],
        [
          "vision_tower.layernorm",
          "Identity"
        ],
        [
          "multi_modal_projector",
          "InternVLMultiModalProjector"
        ],
        [
          "multi_modal_projector.layer_norm",
          "LayerNorm"
        ],
        [
          "multi_modal_projector.linear_1",
          "Linear"
        ],
        [
          "multi_modal_projector.act",
          "GELUActivation"
        ],
        [
          "multi_modal_projector.linear_2",
          "Linear"
        ],
        [
          "language_model",
          "Qwen3Model"
        ],
        [
          "language_model.embed_tokens",
          "Embedding"
        ],
        [
          "language_model.layers",
          "ModuleList"
        ],
        [
          "language_model.layers.0",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.0.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.0.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.0.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.0.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.1",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.1.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.1.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.1.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.1.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.2",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.2.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.2.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.2.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.2.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.3",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.3.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.3.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.3.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.3.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.4",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.4.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.4.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.4.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.4.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.5",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.5.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.5.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.5.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.5.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.6",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.6.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.6.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.6.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.6.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.7",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.7.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.7.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.7.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.7.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.8",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.8.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.8.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.8.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.8.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.9",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.9.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.9.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.9.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.9.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.10",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.10.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.10.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.10.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.10.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.11",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.11.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.11.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.11.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.11.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.12",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.12.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.12.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.12.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.12.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.13",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.13.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.13.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.13.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.13.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.14",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.14.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.14.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.14.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.14.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.15",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.15.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.15.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.15.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.15.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.16",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.16.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.16.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.16.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.16.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.17",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.17.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.17.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.17.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.17.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.18",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.18.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.18.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.18.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.18.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.19",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.19.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.19.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.19.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.19.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.20",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.20.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.20.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.20.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.20.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.21",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.21.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.21.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.21.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.21.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.22",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.22.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.22.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.22.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.22.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.23",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.23.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.23.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.23.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.23.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.24",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.24.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.24.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.24.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.24.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.25",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.25.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.25.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.25.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.25.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.26",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.26.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.26.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.26.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.26.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.27",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.27.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.27.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.27.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.27.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.28",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.28.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.28.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.28.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.28.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.29",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.29.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.29.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.29.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.29.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.30",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.30.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.30.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.30.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.30.post_attention_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.31",
          "Qwen3DecoderLayer"
        ],
        [
          "language_model.layers.31.self_attn",
          "Qwen3Attention"
        ],
        [
          "language_model.layers.31.mlp",
          "Qwen3MLP"
        ],
        [
          "language_model.layers.31.input_layernorm",
          "Qwen3RMSNorm"
        ],
        [
          "language_model.layers.31.post_attention_layernorm",
          "Qwen3RMSNorm"
        ]
      ],
      "module_hits": {
        "vision_like": [
          [
            "vision_tower",
            "InternVLVisionModel"
          ],
          [
            "vision_tower.embeddings",
            "InternVLVisionEmbeddings"
          ],
          [
            "vision_tower.embeddings.patch_embeddings",
            "InternVLVisionPatchEmbeddings"
          ],
          [
            "vision_tower.embeddings.patch_embeddings.projection",
            "Conv2d"
          ],
          [
            "vision_tower.embeddings.dropout",
            "Dropout"
          ],
          [
            "vision_tower.encoder",
            "InternVLVisionEncoder"
          ],
          [
            "vision_tower.encoder.layer",
            "ModuleList"
          ],
          [
            "vision_tower.encoder.layer.0",
            "InternVLVisionLayer"
          ],
          [
            "vision_tower.encoder.layer.0.attention",
            "InternVLVisionAttention"
          ],
          [
            "vision_tower.encoder.layer.0.attention.q_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.0.attention.k_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.0.attention.v_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.0.attention.projection_layer",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.0.attention.projection_dropout",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.0.attention.q_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.0.attention.k_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.0.mlp",
            "InternVLVisionMLP"
          ],
          [
            "vision_tower.encoder.layer.0.mlp.activation_fn",
            "GELUActivation"
          ],
          [
            "vision_tower.encoder.layer.0.mlp.fc1",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.0.mlp.fc2",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.0.layernorm_before",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.0.layernorm_after",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.0.dropout",
            "Dropout"
          ],
          [
            "vision_tower.encoder.layer.1",
            "InternVLVisionLayer"
          ],
          [
            "vision_tower.encoder.layer.1.attention",
            "InternVLVisionAttention"
          ],
          [
            "vision_tower.encoder.layer.1.attention.q_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.1.attention.k_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.1.attention.v_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.1.attention.projection_layer",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.1.attention.projection_dropout",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.1.attention.q_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.1.attention.k_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.1.mlp",
            "InternVLVisionMLP"
          ],
          [
            "vision_tower.encoder.layer.1.mlp.activation_fn",
            "GELUActivation"
          ],
          [
            "vision_tower.encoder.layer.1.mlp.fc1",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.1.mlp.fc2",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.1.layernorm_before",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.1.layernorm_after",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.1.dropout",
            "Dropout"
          ],
          [
            "vision_tower.encoder.layer.2",
            "InternVLVisionLayer"
          ],
          [
            "vision_tower.encoder.layer.2.attention",
            "InternVLVisionAttention"
          ],
          [
            "vision_tower.encoder.layer.2.attention.q_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.2.attention.k_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.2.attention.v_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.2.attention.projection_layer",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.2.attention.projection_dropout",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.2.attention.q_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.2.attention.k_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.2.mlp",
            "InternVLVisionMLP"
          ],
          [
            "vision_tower.encoder.layer.2.mlp.activation_fn",
            "GELUActivation"
          ],
          [
            "vision_tower.encoder.layer.2.mlp.fc1",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.2.mlp.fc2",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.2.layernorm_before",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.2.layernorm_after",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.2.dropout",
            "Dropout"
          ],
          [
            "vision_tower.encoder.layer.3",
            "InternVLVisionLayer"
          ],
          [
            "vision_tower.encoder.layer.3.attention",
            "InternVLVisionAttention"
          ],
          [
            "vision_tower.encoder.layer.3.attention.q_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.3.attention.k_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.3.attention.v_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.3.attention.projection_layer",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.3.attention.projection_dropout",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.3.attention.q_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.3.attention.k_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.3.mlp",
            "InternVLVisionMLP"
          ],
          [
            "vision_tower.encoder.layer.3.mlp.activation_fn",
            "GELUActivation"
          ],
          [
            "vision_tower.encoder.layer.3.mlp.fc1",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.3.mlp.fc2",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.3.layernorm_before",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.3.layernorm_after",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.3.dropout",
            "Dropout"
          ],
          [
            "vision_tower.encoder.layer.4",
            "InternVLVisionLayer"
          ],
          [
            "vision_tower.encoder.layer.4.attention",
            "InternVLVisionAttention"
          ],
          [
            "vision_tower.encoder.layer.4.attention.q_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.4.attention.k_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.4.attention.v_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.4.attention.projection_layer",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.4.attention.projection_dropout",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.4.attention.q_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.4.attention.k_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.4.mlp",
            "InternVLVisionMLP"
          ],
          [
            "vision_tower.encoder.layer.4.mlp.activation_fn",
            "GELUActivation"
          ],
          [
            "vision_tower.encoder.layer.4.mlp.fc1",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.4.mlp.fc2",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.4.layernorm_before",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.4.layernorm_after",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.4.dropout",
            "Dropout"
          ],
          [
            "vision_tower.encoder.layer.5",
            "InternVLVisionLayer"
          ],
          [
            "vision_tower.encoder.layer.5.attention",
            "InternVLVisionAttention"
          ],
          [
            "vision_tower.encoder.layer.5.attention.q_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.5.attention.k_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.5.attention.v_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.5.attention.projection_layer",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.5.attention.projection_dropout",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.5.attention.q_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.5.attention.k_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.5.mlp",
            "InternVLVisionMLP"
          ],
          [
            "vision_tower.encoder.layer.5.mlp.activation_fn",
            "GELUActivation"
          ],
          [
            "vision_tower.encoder.layer.5.mlp.fc1",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.5.mlp.fc2",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.5.layernorm_before",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.5.layernorm_after",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.5.dropout",
            "Dropout"
          ],
          [
            "vision_tower.encoder.layer.6",
            "InternVLVisionLayer"
          ],
          [
            "vision_tower.encoder.layer.6.attention",
            "InternVLVisionAttention"
          ],
          [
            "vision_tower.encoder.layer.6.attention.q_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.6.attention.k_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.6.attention.v_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.6.attention.projection_layer",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.6.attention.projection_dropout",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.6.attention.q_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.6.attention.k_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.6.mlp",
            "InternVLVisionMLP"
          ],
          [
            "vision_tower.encoder.layer.6.mlp.activation_fn",
            "GELUActivation"
          ],
          [
            "vision_tower.encoder.layer.6.mlp.fc1",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.6.mlp.fc2",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.6.layernorm_before",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.6.layernorm_after",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.6.dropout",
            "Dropout"
          ],
          [
            "vision_tower.encoder.layer.7",
            "InternVLVisionLayer"
          ],
          [
            "vision_tower.encoder.layer.7.attention",
            "InternVLVisionAttention"
          ],
          [
            "vision_tower.encoder.layer.7.attention.q_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.7.attention.k_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.7.attention.v_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.7.attention.projection_layer",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.7.attention.projection_dropout",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.7.attention.q_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.7.attention.k_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.7.mlp",
            "InternVLVisionMLP"
          ],
          [
            "vision_tower.encoder.layer.7.mlp.activation_fn",
            "GELUActivation"
          ],
          [
            "vision_tower.encoder.layer.7.mlp.fc1",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.7.mlp.fc2",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.7.layernorm_before",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.7.layernorm_after",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.7.dropout",
            "Dropout"
          ],
          [
            "vision_tower.encoder.layer.8",
            "InternVLVisionLayer"
          ],
          [
            "vision_tower.encoder.layer.8.attention",
            "InternVLVisionAttention"
          ],
          [
            "vision_tower.encoder.layer.8.attention.q_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.8.attention.k_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.8.attention.v_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.8.attention.projection_layer",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.8.attention.projection_dropout",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.8.attention.q_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.8.attention.k_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.8.mlp",
            "InternVLVisionMLP"
          ],
          [
            "vision_tower.encoder.layer.8.mlp.activation_fn",
            "GELUActivation"
          ],
          [
            "vision_tower.encoder.layer.8.mlp.fc1",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.8.mlp.fc2",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.8.layernorm_before",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.8.layernorm_after",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.8.dropout",
            "Dropout"
          ],
          [
            "vision_tower.encoder.layer.9",
            "InternVLVisionLayer"
          ],
          [
            "vision_tower.encoder.layer.9.attention",
            "InternVLVisionAttention"
          ],
          [
            "vision_tower.encoder.layer.9.attention.q_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.9.attention.k_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.9.attention.v_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.9.attention.projection_layer",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.9.attention.projection_dropout",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.9.attention.q_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.9.attention.k_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.9.mlp",
            "InternVLVisionMLP"
          ],
          [
            "vision_tower.encoder.layer.9.mlp.activation_fn",
            "GELUActivation"
          ],
          [
            "vision_tower.encoder.layer.9.mlp.fc1",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.9.mlp.fc2",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.9.layernorm_before",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.9.layernorm_after",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.9.dropout",
            "Dropout"
          ],
          [
            "vision_tower.encoder.layer.10",
            "InternVLVisionLayer"
          ],
          [
            "vision_tower.encoder.layer.10.attention",
            "InternVLVisionAttention"
          ],
          [
            "vision_tower.encoder.layer.10.attention.q_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.10.attention.k_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.10.attention.v_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.10.attention.projection_layer",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.10.attention.projection_dropout",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.10.attention.q_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.10.attention.k_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.10.mlp",
            "InternVLVisionMLP"
          ],
          [
            "vision_tower.encoder.layer.10.mlp.activation_fn",
            "GELUActivation"
          ],
          [
            "vision_tower.encoder.layer.10.mlp.fc1",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.10.mlp.fc2",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.10.layernorm_before",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.10.layernorm_after",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.10.dropout",
            "Dropout"
          ],
          [
            "vision_tower.encoder.layer.11",
            "InternVLVisionLayer"
          ],
          [
            "vision_tower.encoder.layer.11.attention",
            "InternVLVisionAttention"
          ],
          [
            "vision_tower.encoder.layer.11.attention.q_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.11.attention.k_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.11.attention.v_proj",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.11.attention.projection_layer",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.11.attention.projection_dropout",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.11.attention.q_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.11.attention.k_norm",
            "Identity"
          ],
          [
            "vision_tower.encoder.layer.11.mlp",
            "InternVLVisionMLP"
          ],
          [
            "vision_tower.encoder.layer.11.mlp.activation_fn",
            "GELUActivation"
          ],
          [
            "vision_tower.encoder.layer.11.mlp.fc1",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.11.mlp.fc2",
            "Linear"
          ],
          [
            "vision_tower.encoder.layer.11.layernorm_before",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.11.layernorm_after",
            "LayerNorm"
          ],
          [
            "vision_tower.encoder.layer.11.dropout",
            "Dropout"
          ],
          [
            "vision_tower.encoder.layer.12",
            "InternVLVisionLayer"
          ]
        ],
        "projector_like": [
          [
            "multi_modal_projector",
            "InternVLMultiModalProjector"
          ],
          [
            "multi_modal_projector.layer_norm",
            "LayerNorm"
          ],
          [
            "multi_modal_projector.linear_1",
            "Linear"
          ],
          [
            "multi_modal_projector.act",
            "GELUActivation"
          ],
          [
            "multi_modal_projector.linear_2",
            "Linear"
          ]
        ],
        "fusion_like": [],
        "adapter_like": []
      },
      "hook_candidates_registered": {
        "vision": [
          "vision_tower",
          "vision_tower.embeddings",
          "vision_tower.embeddings.patch_embeddings",
          "vision_tower.embeddings.patch_embeddings.projection",
          "vision_tower.embeddings.dropout",
          "vision_tower.encoder",
          "vision_tower.encoder.layer",
          "vision_tower.encoder.layer.0",
          "vision_tower.encoder.layer.0.attention",
          "vision_tower.encoder.layer.0.attention.q_proj",
          "vision_tower.encoder.layer.0.attention.k_proj",
          "vision_tower.encoder.layer.0.attention.v_proj",
          "vision_tower.encoder.layer.0.attention.projection_layer",
          "vision_tower.encoder.layer.0.attention.projection_dropout",
          "vision_tower.encoder.layer.0.attention.q_norm",
          "vision_tower.encoder.layer.0.attention.k_norm",
          "vision_tower.encoder.layer.0.mlp",
          "vision_tower.encoder.layer.0.mlp.activation_fn",
          "vision_tower.encoder.layer.0.mlp.fc1",
          "vision_tower.encoder.layer.0.mlp.fc2",
          "vision_tower.encoder.layer.0.layernorm_before",
          "vision_tower.encoder.layer.0.layernorm_after",
          "vision_tower.encoder.layer.0.dropout",
          "vision_tower.encoder.layer.1",
          "vision_tower.encoder.layer.1.attention",
          "vision_tower.encoder.layer.1.attention.q_proj",
          "vision_tower.encoder.layer.1.attention.k_proj",
          "vision_tower.encoder.layer.1.attention.v_proj",
          "vision_tower.encoder.layer.1.attention.projection_layer",
          "vision_tower.encoder.layer.1.attention.projection_dropout"
        ],
        "projector": [
          "multi_modal_projector",
          "multi_modal_projector.linear_2"
        ],
        "adapter": [],
        "fusion": []
      },
      "hook_fired_summary": [
        {
          "name": "vision:vision_tower.embeddings.patch_embeddings.projection",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1024,
              32,
              32
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -3.359375,
            "max": 3.421875,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.embeddings.patch_embeddings",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1024,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -3.359375,
            "max": 3.421875,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.embeddings.dropout",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -3.375,
            "max": 3.4375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.embeddings",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -3.375,
            "max": 3.4375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.0.layernorm_before",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -4.6875,
            "max": 5.28125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.0.attention.q_proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -3.109375,
            "max": 3.359375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.0.attention.k_proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -4.09375,
            "max": 3.4375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.0.attention.v_proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -2.390625,
            "max": 2.25,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.0.attention.q_norm",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -3.109375,
            "max": 3.359375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.0.attention.k_norm",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -4.09375,
            "max": 3.4375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.0.attention.projection_layer",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -1.640625,
            "max": 1.9609375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.0.attention.projection_dropout",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -1.640625,
            "max": 1.9609375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.0.attention",
          "calls": 1,
          "first_output": {
            "type": "tuple",
            "len": 2,
            "items": [
              {
                "i": 0,
                "shape": [
                  1,
                  1025,
                  1024
                ],
                "dtype": "bfloat16",
                "device": "cuda:0",
                "min": -1.640625,
                "max": 1.9609375,
                "nan": 0,
                "inf": 0
              },
              {
                "i": 1,
                "type": "NoneType"
              }
            ]
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.0.layernorm_after",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -9.125,
            "max": 16.5,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.0.mlp.fc1",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              4096
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -6.96875,
            "max": 4.5,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.0.mlp.activation_fn",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              4096
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -0.169921875,
            "max": 4.5,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.0.mlp.fc2",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -2.3125,
            "max": 2.3125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.0.mlp",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -2.3125,
            "max": 2.3125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.0.dropout",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -2.3125,
            "max": 2.3125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.0",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -4.59375,
            "max": 3.203125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.1.attention.q_proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -3.53125,
            "max": 3.546875,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.1.attention.k_proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -2.921875,
            "max": 3.25,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.1.attention.v_proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -2.359375,
            "max": 2.3125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.1.attention.projection_layer",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -0.73046875,
            "max": 0.8125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.1.attention.projection_dropout",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -0.73046875,
            "max": 0.8125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.1.attention",
          "calls": 1,
          "first_output": {
            "type": "tuple",
            "len": 2,
            "items": [
              {
                "i": 0,
                "shape": [
                  1,
                  1025,
                  1024
                ],
                "dtype": "bfloat16",
                "device": "cuda:0",
                "min": -0.73046875,
                "max": 0.8125,
                "nan": 0,
                "inf": 0
              },
              {
                "i": 1,
                "type": "NoneType"
              }
            ]
          }
        },
        {
          "name": "vision:vision_tower.encoder.layer.1",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              1025,
              1024
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -6.5,
            "max": 4.3125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:vision_tower.encoder",
          "calls": 1,
          "first_output": {
            "type": "dict",
            "keys": [
              "last_hidden_state"
            ],
            "tensors": {
              "last_hidden_state": {
                "shape": [
                  1,
                  1025,
                  1024
                ],
                "dtype": "bfloat16",
                "device": "cuda:0",
                "min": -104.0,
                "max": 158.0,
                "nan": 0,
                "inf": 0
              }
            }
          }
        },
        {
          "name": "vision:vision_tower",
          "calls": 1,
          "first_output": {
            "type": "dict",
            "keys": [
              "last_hidden_state"
            ],
            "tensors": {
              "last_hidden_state": {
                "shape": [
                  1,
                  1025,
                  1024
                ],
                "dtype": "bfloat16",
                "device": "cuda:0",
                "min": -104.0,
                "max": 158.0,
                "nan": 0,
                "inf": 0
              }
            }
          }
        },
        {
          "name": "projector:multi_modal_projector.linear_2",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              256,
              4096
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -2.75,
            "max": 4.8125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "projector:multi_modal_projector",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              1,
              256,
              4096
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -2.75,
            "max": 4.8125,
            "nan": 0,
            "inf": 0
          }
        }
      ],
      "hookable": {
        "vision_path_hookable": true,
        "projector_path_hookable": true
      },
      "forward_kwargs_tried": {
        "used_kwargs": {
          "output_hidden_states": true,
          "output_attentions": true,
          "return_dict": true
        }
      },
      "top_level_outputs": {
        "type": "InternVLModelOutputWithPast",
        "last_hidden_state": {
          "type": "tensor",
          "shape": [
            1,
            291,
            4096
          ],
          "dtype": "bfloat16",
          "device": "cuda:0",
          "min": -93.0,
          "max": 74.0,
          "nan": 0,
          "inf": 0
        },
        "hidden_states": {
          "type": "tuple",
          "len": 37,
          "last": {
            "type": "tensor",
            "shape": [
              1,
              291,
              4096
            ],
            "dtype": "bfloat16",
            "device": "cuda:0",
            "min": -93.0,
            "max": 74.0,
            "nan": 0,
            "inf": 0
          }
        }
      },
      "notes": {
        "vision_encoder_extractable": "LIKELY",
        "projector_extractable": "LIKELY"
      }
    },
    {
      "backend": "lingshu",
      "model_id": "lingshu-medical-mllm/Lingshu-7B",
      "device": "cuda",
      "dtype_first_param": "float16",
      "tree_depth": 3,
      "tree_top": [
        [
          "model",
          "Qwen2_5_VLModel"
        ],
        [
          "model.visual",
          "Qwen2_5_VisionTransformerPretrainedModel"
        ],
        [
          "model.visual.patch_embed",
          "Qwen2_5_VisionPatchEmbed"
        ],
        [
          "model.visual.patch_embed.proj",
          "Conv3d"
        ],
        [
          "model.visual.rotary_pos_emb",
          "Qwen2_5_VisionRotaryEmbedding"
        ],
        [
          "model.visual.blocks",
          "ModuleList"
        ],
        [
          "model.visual.blocks.0",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.1",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.2",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.3",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.4",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.5",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.6",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.7",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.8",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.9",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.10",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.11",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.12",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.13",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.14",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.15",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.16",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.17",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.18",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.19",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.20",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.21",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.22",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.23",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.24",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.25",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.26",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.27",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.28",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.29",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.30",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.blocks.31",
          "Qwen2_5_VLVisionBlock"
        ],
        [
          "model.visual.merger",
          "Qwen2_5_VLPatchMerger"
        ],
        [
          "model.visual.merger.ln_q",
          "Qwen2RMSNorm"
        ],
        [
          "model.visual.merger.mlp",
          "Sequential"
        ],
        [
          "model.language_model",
          "Qwen2_5_VLTextModel"
        ],
        [
          "model.language_model.embed_tokens",
          "Embedding"
        ],
        [
          "model.language_model.layers",
          "ModuleList"
        ],
        [
          "model.language_model.layers.0",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.1",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.2",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.3",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.4",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.5",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.6",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.7",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.8",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.9",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.10",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.11",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.12",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.13",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.14",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.15",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.16",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.17",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.18",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.19",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.20",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.21",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.22",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.23",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.24",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.25",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.26",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.layers.27",
          "Qwen2_5_VLDecoderLayer"
        ],
        [
          "model.language_model.norm",
          "Qwen2RMSNorm"
        ],
        [
          "model.language_model.rotary_emb",
          "Qwen2_5_VLRotaryEmbedding"
        ],
        [
          "lm_head",
          "Linear"
        ]
      ],
      "module_hits": {
        "vision_like": [
          [
            "model.visual",
            "Qwen2_5_VisionTransformerPretrainedModel"
          ],
          [
            "model.visual.patch_embed",
            "Qwen2_5_VisionPatchEmbed"
          ],
          [
            "model.visual.patch_embed.proj",
            "Conv3d"
          ],
          [
            "model.visual.rotary_pos_emb",
            "Qwen2_5_VisionRotaryEmbedding"
          ],
          [
            "model.visual.blocks",
            "ModuleList"
          ],
          [
            "model.visual.blocks.0",
            "Qwen2_5_VLVisionBlock"
          ],
          [
            "model.visual.blocks.0.norm1",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.0.norm2",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.0.attn",
            "Qwen2_5_VLVisionAttention"
          ],
          [
            "model.visual.blocks.0.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.0.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.0.mlp",
            "Qwen2_5_VLMLP"
          ],
          [
            "model.visual.blocks.0.mlp.gate_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.0.mlp.up_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.0.mlp.down_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.0.mlp.act_fn",
            "SiLUActivation"
          ],
          [
            "model.visual.blocks.1",
            "Qwen2_5_VLVisionBlock"
          ],
          [
            "model.visual.blocks.1.norm1",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.1.norm2",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.1.attn",
            "Qwen2_5_VLVisionAttention"
          ],
          [
            "model.visual.blocks.1.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.1.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.1.mlp",
            "Qwen2_5_VLMLP"
          ],
          [
            "model.visual.blocks.1.mlp.gate_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.1.mlp.up_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.1.mlp.down_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.1.mlp.act_fn",
            "SiLUActivation"
          ],
          [
            "model.visual.blocks.2",
            "Qwen2_5_VLVisionBlock"
          ],
          [
            "model.visual.blocks.2.norm1",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.2.norm2",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.2.attn",
            "Qwen2_5_VLVisionAttention"
          ],
          [
            "model.visual.blocks.2.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.2.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.2.mlp",
            "Qwen2_5_VLMLP"
          ],
          [
            "model.visual.blocks.2.mlp.gate_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.2.mlp.up_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.2.mlp.down_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.2.mlp.act_fn",
            "SiLUActivation"
          ],
          [
            "model.visual.blocks.3",
            "Qwen2_5_VLVisionBlock"
          ],
          [
            "model.visual.blocks.3.norm1",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.3.norm2",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.3.attn",
            "Qwen2_5_VLVisionAttention"
          ],
          [
            "model.visual.blocks.3.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.3.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.3.mlp",
            "Qwen2_5_VLMLP"
          ],
          [
            "model.visual.blocks.3.mlp.gate_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.3.mlp.up_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.3.mlp.down_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.3.mlp.act_fn",
            "SiLUActivation"
          ],
          [
            "model.visual.blocks.4",
            "Qwen2_5_VLVisionBlock"
          ],
          [
            "model.visual.blocks.4.norm1",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.4.norm2",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.4.attn",
            "Qwen2_5_VLVisionAttention"
          ],
          [
            "model.visual.blocks.4.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.4.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.4.mlp",
            "Qwen2_5_VLMLP"
          ],
          [
            "model.visual.blocks.4.mlp.gate_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.4.mlp.up_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.4.mlp.down_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.4.mlp.act_fn",
            "SiLUActivation"
          ],
          [
            "model.visual.blocks.5",
            "Qwen2_5_VLVisionBlock"
          ],
          [
            "model.visual.blocks.5.norm1",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.5.norm2",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.5.attn",
            "Qwen2_5_VLVisionAttention"
          ],
          [
            "model.visual.blocks.5.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.5.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.5.mlp",
            "Qwen2_5_VLMLP"
          ],
          [
            "model.visual.blocks.5.mlp.gate_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.5.mlp.up_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.5.mlp.down_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.5.mlp.act_fn",
            "SiLUActivation"
          ],
          [
            "model.visual.blocks.6",
            "Qwen2_5_VLVisionBlock"
          ],
          [
            "model.visual.blocks.6.norm1",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.6.norm2",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.6.attn",
            "Qwen2_5_VLVisionAttention"
          ],
          [
            "model.visual.blocks.6.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.6.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.6.mlp",
            "Qwen2_5_VLMLP"
          ],
          [
            "model.visual.blocks.6.mlp.gate_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.6.mlp.up_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.6.mlp.down_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.6.mlp.act_fn",
            "SiLUActivation"
          ],
          [
            "model.visual.blocks.7",
            "Qwen2_5_VLVisionBlock"
          ],
          [
            "model.visual.blocks.7.norm1",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.7.norm2",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.7.attn",
            "Qwen2_5_VLVisionAttention"
          ],
          [
            "model.visual.blocks.7.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.7.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.7.mlp",
            "Qwen2_5_VLMLP"
          ],
          [
            "model.visual.blocks.7.mlp.gate_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.7.mlp.up_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.7.mlp.down_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.7.mlp.act_fn",
            "SiLUActivation"
          ],
          [
            "model.visual.blocks.8",
            "Qwen2_5_VLVisionBlock"
          ],
          [
            "model.visual.blocks.8.norm1",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.8.norm2",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.8.attn",
            "Qwen2_5_VLVisionAttention"
          ],
          [
            "model.visual.blocks.8.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.8.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.8.mlp",
            "Qwen2_5_VLMLP"
          ],
          [
            "model.visual.blocks.8.mlp.gate_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.8.mlp.up_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.8.mlp.down_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.8.mlp.act_fn",
            "SiLUActivation"
          ],
          [
            "model.visual.blocks.9",
            "Qwen2_5_VLVisionBlock"
          ],
          [
            "model.visual.blocks.9.norm1",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.9.norm2",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.9.attn",
            "Qwen2_5_VLVisionAttention"
          ],
          [
            "model.visual.blocks.9.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.9.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.9.mlp",
            "Qwen2_5_VLMLP"
          ],
          [
            "model.visual.blocks.9.mlp.gate_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.9.mlp.up_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.9.mlp.down_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.9.mlp.act_fn",
            "SiLUActivation"
          ],
          [
            "model.visual.blocks.10",
            "Qwen2_5_VLVisionBlock"
          ],
          [
            "model.visual.blocks.10.norm1",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.10.norm2",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.10.attn",
            "Qwen2_5_VLVisionAttention"
          ],
          [
            "model.visual.blocks.10.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.10.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.10.mlp",
            "Qwen2_5_VLMLP"
          ],
          [
            "model.visual.blocks.10.mlp.gate_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.10.mlp.up_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.10.mlp.down_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.10.mlp.act_fn",
            "SiLUActivation"
          ],
          [
            "model.visual.blocks.11",
            "Qwen2_5_VLVisionBlock"
          ],
          [
            "model.visual.blocks.11.norm1",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.11.norm2",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.11.attn",
            "Qwen2_5_VLVisionAttention"
          ],
          [
            "model.visual.blocks.11.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.11.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.11.mlp",
            "Qwen2_5_VLMLP"
          ],
          [
            "model.visual.blocks.11.mlp.gate_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.11.mlp.up_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.11.mlp.down_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.11.mlp.act_fn",
            "SiLUActivation"
          ],
          [
            "model.visual.blocks.12",
            "Qwen2_5_VLVisionBlock"
          ],
          [
            "model.visual.blocks.12.norm1",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.12.norm2",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.12.attn",
            "Qwen2_5_VLVisionAttention"
          ],
          [
            "model.visual.blocks.12.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.12.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.12.mlp",
            "Qwen2_5_VLMLP"
          ],
          [
            "model.visual.blocks.12.mlp.gate_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.12.mlp.up_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.12.mlp.down_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.12.mlp.act_fn",
            "SiLUActivation"
          ],
          [
            "model.visual.blocks.13",
            "Qwen2_5_VLVisionBlock"
          ],
          [
            "model.visual.blocks.13.norm1",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.13.norm2",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.13.attn",
            "Qwen2_5_VLVisionAttention"
          ],
          [
            "model.visual.blocks.13.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.13.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.13.mlp",
            "Qwen2_5_VLMLP"
          ],
          [
            "model.visual.blocks.13.mlp.gate_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.13.mlp.up_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.13.mlp.down_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.13.mlp.act_fn",
            "SiLUActivation"
          ],
          [
            "model.visual.blocks.14",
            "Qwen2_5_VLVisionBlock"
          ],
          [
            "model.visual.blocks.14.norm1",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.14.norm2",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.14.attn",
            "Qwen2_5_VLVisionAttention"
          ],
          [
            "model.visual.blocks.14.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.14.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.14.mlp",
            "Qwen2_5_VLMLP"
          ],
          [
            "model.visual.blocks.14.mlp.gate_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.14.mlp.up_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.14.mlp.down_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.14.mlp.act_fn",
            "SiLUActivation"
          ],
          [
            "model.visual.blocks.15",
            "Qwen2_5_VLVisionBlock"
          ],
          [
            "model.visual.blocks.15.norm1",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.15.norm2",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.15.attn",
            "Qwen2_5_VLVisionAttention"
          ],
          [
            "model.visual.blocks.15.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.15.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.15.mlp",
            "Qwen2_5_VLMLP"
          ],
          [
            "model.visual.blocks.15.mlp.gate_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.15.mlp.up_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.15.mlp.down_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.15.mlp.act_fn",
            "SiLUActivation"
          ],
          [
            "model.visual.blocks.16",
            "Qwen2_5_VLVisionBlock"
          ],
          [
            "model.visual.blocks.16.norm1",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.16.norm2",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.16.attn",
            "Qwen2_5_VLVisionAttention"
          ],
          [
            "model.visual.blocks.16.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.16.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.16.mlp",
            "Qwen2_5_VLMLP"
          ],
          [
            "model.visual.blocks.16.mlp.gate_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.16.mlp.up_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.16.mlp.down_proj",
            "Linear"
          ],
          [
            "model.visual.blocks.16.mlp.act_fn",
            "SiLUActivation"
          ],
          [
            "model.visual.blocks.17",
            "Qwen2_5_VLVisionBlock"
          ],
          [
            "model.visual.blocks.17.norm1",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.17.norm2",
            "Qwen2RMSNorm"
          ],
          [
            "model.visual.blocks.17.attn",
            "Qwen2_5_VLVisionAttention"
          ],
          [
            "model.visual.blocks.17.attn.qkv",
            "Linear"
          ],
          [
            "model.visual.blocks.17.attn.proj",
            "Linear"
          ],
          [
            "model.visual.blocks.17.mlp",
            "Qwen2_5_VLMLP"
          ],
          [
            "model.visual.blocks.17.mlp.gate_proj",
            "Linear"
          ]
        ],
        "projector_like": [],
        "fusion_like": [],
        "adapter_like": []
      },
      "hook_candidates_registered": {
        "vision": [
          "model.visual",
          "model.visual.patch_embed",
          "model.visual.patch_embed.proj",
          "model.visual.rotary_pos_emb",
          "model.visual.blocks",
          "model.visual.blocks.0",
          "model.visual.blocks.0.norm1",
          "model.visual.blocks.0.norm2",
          "model.visual.blocks.0.attn",
          "model.visual.blocks.0.attn.qkv",
          "model.visual.blocks.0.attn.proj",
          "model.visual.blocks.0.mlp",
          "model.visual.blocks.0.mlp.gate_proj",
          "model.visual.blocks.0.mlp.up_proj",
          "model.visual.blocks.0.mlp.down_proj",
          "model.visual.blocks.0.mlp.act_fn",
          "model.visual.blocks.1",
          "model.visual.blocks.1.norm1",
          "model.visual.blocks.1.norm2",
          "model.visual.blocks.1.attn",
          "model.visual.blocks.1.attn.qkv",
          "model.visual.blocks.1.attn.proj",
          "model.visual.blocks.1.mlp",
          "model.visual.blocks.1.mlp.gate_proj",
          "model.visual.blocks.1.mlp.up_proj",
          "model.visual.blocks.1.mlp.down_proj",
          "model.visual.blocks.1.mlp.act_fn",
          "model.visual.blocks.2",
          "model.visual.blocks.2.norm1",
          "model.visual.blocks.2.norm2"
        ],
        "projector": [],
        "adapter": [],
        "fusion": []
      },
      "hook_fired_summary": [
        {
          "name": "vision:model.visual.patch_embed.proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1280,
              1,
              1,
              1
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -4.671875,
            "max": 5.2734375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.patch_embed",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1280
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -4.671875,
            "max": 5.2734375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.rotary_pos_emb",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              16,
              20
            ],
            "dtype": "float32",
            "device": "cuda:0",
            "min": 0.0,
            "max": 15.0,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.0.norm1",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1280
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -1.712890625,
            "max": 1.76171875,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.0.attn.qkv",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              3840
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -3.564453125,
            "max": 4.26953125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.0.attn.proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1280
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -1.6669921875,
            "max": 0.77685546875,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.0.attn",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1280
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -1.6669921875,
            "max": 0.77685546875,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.0.norm2",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1280
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -2.810546875,
            "max": 3.1015625,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.0.mlp.gate_proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              3420
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -9.6171875,
            "max": 8.1484375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.0.mlp.act_fn",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              3420
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -0.278564453125,
            "max": 8.1484375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.0.mlp.up_proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              3420
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -10.40625,
            "max": 11.34375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.0.mlp.down_proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1280
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -13.96875,
            "max": 5.40625,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.0.mlp",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1280
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -13.96875,
            "max": 5.40625,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.0",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1280
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -15.484375,
            "max": 5.453125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.1.norm1",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1280
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -3.876953125,
            "max": 2.318359375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.1.attn.qkv",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              3840
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -4.15625,
            "max": 4.58203125,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.1.attn.proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1280
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -1.2353515625,
            "max": 1.6484375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.1.attn",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1280
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -1.2353515625,
            "max": 1.6484375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.1.norm2",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1280
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -4.51171875,
            "max": 4.69921875,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.1.mlp.gate_proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              3420
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -4.11328125,
            "max": 3.43359375,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.1.mlp.act_fn",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              3420
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -0.278564453125,
            "max": 3.326171875,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.1.mlp.up_proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              3420
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -3.228515625,
            "max": 3.998046875,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.1.mlp.down_proj",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1280
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -4.71875,
            "max": 1.9248046875,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.1.mlp",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1280
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -4.71875,
            "max": 1.9248046875,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.1",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1280
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -19.15625,
            "max": 6.1015625,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.2.norm1",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1280
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -4.8359375,
            "max": 3.22265625,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.2.norm2",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1280
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -3.666015625,
            "max": 4.0390625,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual.blocks.2",
          "calls": 1,
          "first_output": {
            "type": "tensor",
            "shape": [
              256,
              1280
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -22.296875,
            "max": 6.12890625,
            "nan": 0,
            "inf": 0
          }
        },
        {
          "name": "vision:model.visual",
          "calls": 1,
          "first_output": {
            "type": "dict",
            "keys": [
              "last_hidden_state",
              "pooler_output"
            ],
            "tensors": {
              "last_hidden_state": {
                "shape": [
                  256,
                  1280
                ],
                "dtype": "float16",
                "device": "cuda:0",
                "min": -39040.0,
                "max": 2978.0,
                "nan": 0,
                "inf": 0
              },
              "pooler_output": {
                "shape": [
                  64,
                  3584
                ],
                "dtype": "float16",
                "device": "cuda:0",
                "min": -13.046875,
                "max": 19.296875,
                "nan": 0,
                "inf": 0
              }
            }
          }
        }
      ],
      "hookable": {
        "vision_path_hookable": true,
        "projector_path_hookable": false
      },
      "forward_kwargs_tried": {
        "used_kwargs": {
          "output_hidden_states": true,
          "output_attentions": true,
          "return_dict": true
        }
      },
      "top_level_outputs": {
        "type": "Qwen2_5_VLCausalLMOutputWithPast",
        "hidden_states": {
          "type": "tuple",
          "len": 29,
          "last": {
            "type": "tensor",
            "shape": [
              1,
              114,
              3584
            ],
            "dtype": "float16",
            "device": "cuda:0",
            "min": -221.0,
            "max": 128.625,
            "nan": 0,
            "inf": 0
          }
        },
        "attentions": {
          "type": "tuple",
          "len": 28
        },
        "logits": {
          "type": "tensor",
          "shape": [
            1,
            114,
            152064
          ],
          "dtype": "float16",
          "device": "cuda:0",
          "min": -36.6875,
          "max": 44.5625,
          "nan": 0,
          "inf": 0
        }
      },
      "notes": {
        "vision_encoder_extractable": "LIKELY",
        "projector_extractable": "UNCERTAIN (no projector-related hooks fired; module may be named differently)"
      }
    }
  ]
}