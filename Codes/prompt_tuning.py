import os
import random
import numpy as np
import pandas as pd
from PIL import Image

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW


from typing import List, Dict, Any, Tuple, Optional,Union

# ============================================================
# 0. ê³µí†µ ì„¤ì •
# ============================================================
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

TRAIN_CSV = "prompt_tuning_train_750.csv"
VAL_CSV   = "prompt_tuning_val_150.csv"

# ì´ ë”•ì…”ë„ˆë¦¬/í”„ë¡¬í”„íŠ¸ëŠ” ë„¤ ê¸°ì¡´ ì½”ë“œì—ì„œ ì´ë¯¸ ì •ì˜ë¼ ìˆë‹¤ê³  ê°€ì •
MODEL_ID_BY_BACKEND = { "qwen3": "Qwen/Qwen3-VL-2B-Instruct", "medgemma": "google/medgemma-4b-it", "internvl": "OpenGVLab/InternVL3_5-8B-HF", "lingshu": "lingshu-medical-mllm/Lingshu-7B", }
PROMPT_BY_DATASET = {
    "mri": (
        "This is a brain MRI scan.\n"
        "Question: Does this image show normal anatomy or signs of disease?\n\n"
    ),
    "oct": (
        "This is a retinal OCT scan.\n"
        "Question: Does this image show normal anatomy or signs of disease?\n\n"
    ),
    "xray": (
        "This is a chest X-ray image.\n"
        "Question: Does this image show normal anatomy or signs of disease?\n\n"
    ),
    "fundus": (
        "This is a retinal fundus photograph.\n"
        "Question: Does this image show normal anatomy or signs of disease?\n\n"
    ),
}

SYSTEM_PROMPT_NORMAL = (
    "You are a medical image classifier.\n"
    "Respond using ONLY valid JSON.\n"
    "Output exactly one JSON object with one key called \"label\".\n"
    "The value of \"label\" MUST be either \"normal\" or \"disease\".\n"
    "Do NOT include any explanation, text, or formatting outside the JSON."
)



def load_backend(backend, model_id):
    dtype = torch.bfloat16 if device == "cuda" else torch.float32

    if backend == "qwen3":
        from transformers import Qwen3VLForConditionalGeneration, AutoProcessor
        model = Qwen3VLForConditionalGeneration.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else {"": "cpu"},
        )
        processor = AutoProcessor.from_pretrained(model_id)
        model.eval()
        return model, processor

    if backend in ["medgemma", "internvl"]:
        from transformers import AutoProcessor
        processor = AutoProcessor.from_pretrained(model_id)
        try:
            from transformers import AutoModelForImageTextToText
            model = AutoModelForImageTextToText.from_pretrained(
                model_id,
                torch_dtype=dtype,
                device_map="auto" if device == "cuda" else {"": "cpu"},
            )
        except Exception:
            from transformers import AutoModelForVision2Seq
            model = AutoModelForVision2Seq.from_pretrained(
                model_id,
                torch_dtype=dtype,
                device_map="auto" if device == "cuda" else {"": "cpu"},
            )
        model.eval()
        return model, processor

    if backend == "lingshu":
        from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_id,
            torch_dtype=dtype,
            device_map="auto" if device == "cuda" else {"": "cpu"},
        )
        processor = AutoProcessor.from_pretrained(model_id)
        model.eval()
        return model, processor

    raise ValueError(f"Unknown backend: {backend}")


BACKENDS = ["lingshu", "internvl"]
NUM_VIRTUAL_TOKENS = 20

# CE ê°€ì¤‘ì¹˜ (weakì— ë” í° weight)
w_c = 1.0   # clean
w_w = 2.0   # weak

# consistency ê°€ì¤‘ì¹˜
lambda_w = 0.5  # clean vs weak

LR = 1e-3
EPOCHS = 5
BATCH_SIZE = 4

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if device == "cuda":
    torch.cuda.manual_seed_all(SEED)



# ============================================================
# 1. SoftPrompt ë˜í¼
# ============================================================
class SoftPromptVLM(nn.Module):
    def __init__(self, base_model, num_virtual_tokens: int = 20, keep_input_ids: bool = False):
        super().__init__()
        self.base_model = base_model
        self.num_virtual_tokens = num_virtual_tokens
        self.keep_input_ids = keep_input_ids

        # base_model ì„ë² ë”© ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        input_embeddings = self.base_model.get_input_embeddings()
        hidden_size = input_embeddings.weight.shape[1]
        embed_dtype = input_embeddings.weight.dtype
        embed_device = input_embeddings.weight.device

        # ğŸ”¥ base_model ì„ë² ë”©ê³¼ ë™ì¼í•œ dtype/deviceë¡œ soft_prompt ìƒì„±
        self.soft_prompt = nn.Embedding(
            num_virtual_tokens,
            hidden_size,
            dtype=embed_dtype,
            device=embed_device,
        )

        for p in self.base_model.parameters():
            p.requires_grad = False

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        labels=None,
        output_hidden_states: bool = False,
        **kwargs,
    ):
        if input_ids is None:
            raise ValueError("SoftPromptVLM.forward requires input_ids")

        # ì›ë³¸ í† í° ì„ë² ë”©
        input_embeds = self.base_model.get_input_embeddings()(input_ids)
        B, L, H = input_embeds.shape
        device = input_embeds.device

        # ğŸ”¥ dtype ë§ì¶”ê¸° (í˜¹ì‹œë¼ë„ ë‹¤ë¥´ë©´ soft_prompt ìª½ì— ë§ì¶°ì¤Œ)
        target_dtype = self.soft_prompt.weight.dtype
        if input_embeds.dtype != target_dtype:
            input_embeds = input_embeds.to(target_dtype)

        # soft prompt ì„ë² ë”©
        virtual_token_ids = torch.arange(
            self.num_virtual_tokens, device=device
        ).unsqueeze(0).expand(B, -1)
        soft_embeds = self.soft_prompt(virtual_token_ids)  # (B, V, H)

        # [soft] + [original]
        inputs_embeds = torch.cat([soft_embeds, input_embeds], dim=1)  # (B, V+L, H)

        # attention mask í™•ì¥
        if attention_mask is not None:
            soft_mask = torch.ones(
                (B, self.num_virtual_tokens),
                device=device,
                dtype=attention_mask.dtype,
            )
            attention_mask = torch.cat([soft_mask, attention_mask], dim=1)

        # labels í™•ì¥
        if labels is not None:
            pad = torch.full(
                (B, self.num_virtual_tokens),
                fill_value=-100,
                device=device,
                dtype=labels.dtype,
            )
            labels = torch.cat([pad, labels], dim=1)

        # internvl ê°™ì€ ì• ë“¤ì€ input_ids ì œê±°, lingshuëŠ” ìœ ì§€
        if "input_ids" in kwargs and not self.keep_input_ids:
            kwargs.pop("input_ids")

        outputs = self.base_model(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            labels=labels,
            output_hidden_states=output_hidden_states,
            **kwargs,
        )
        return outputs





# ============================================================
# 2. train/val CSV ë¡œë“œ (clean/weakë§Œ)
# ============================================================
base_out_dir = os.path.expanduser("~/Scratch/vlm_prompt_dataset")
def load_split_csv(path, base_out_dir):
    df = pd.read_csv(path)

    # binarylab â†’ binarylabel
    if "binarylab" in df.columns and "binarylabel" not in df.columns:
        df = df.rename(columns={"binarylab": "binarylabel"})

    # severity / dataset ì •ê·œí™”
    df["severity_norm"] = df["severity"].astype(str).str.lower()
    df["dataset_norm"]  = df["dataset"].astype(str).str.lower()

    # ğŸ”¥ strong ì œì™¸
    df = df[df["severity_norm"].isin(["clean", "weak"])].copy()

    # ğŸ”¥ Windows path â†’ Myriad path ë³€í™˜
    df["filepath"] = (
        df["filepath"]
        .astype(str)
        .str.replace(
            r"C:\Users\hanna\Lectures\Research_Project\Codes\Dataset\vlm_prompt_dataset",
            base_out_dir,
            regex=False,
        )
        .str.replace("\\", "/", regex=False)
    )

    if "fileindex" not in df.columns:
        raise ValueError(f"{path} ì— fileindex ì»¬ëŸ¼ í•„ìš”í•¨")

    return df

train_df = load_split_csv(TRAIN_CSV, base_out_dir)
val_df   = load_split_csv(VAL_CSV, base_out_dir)



class CleanWeakPairDataset(Dataset):
    """
    fileindex ê¸°ì¤€ìœ¼ë¡œ clean / weak í˜ì–´ë¥¼ ê°€ì ¸ì˜¤ëŠ” Dataset.
    """
    def __init__(self, df, processor, prompt_by_dataset, system_prompt=None):
        self.df = df.copy()
        self.processor = processor
        self.prompt_by_dataset = prompt_by_dataset
        self.system_prompt = system_prompt

        self.df["severity_norm"] = self.df["severity"].astype(str).str.lower()
        self.df["dataset_norm"]  = self.df["dataset"].astype(str).str.lower()

        self.pairs = []  # (clean_row, weak_row)

        for fid, g in self.df.groupby("fileindex"):
            g = g.reset_index(drop=True)
            clean_rows = g[g["severity_norm"] == "clean"]
            weak_rows  = g[g["severity_norm"] == "weak"]
            if len(clean_rows) == 0 or len(weak_rows) == 0:
                continue
            self.pairs.append(
                (clean_rows.iloc[0], weak_rows.iloc[0])
            )

        if len(self.pairs) == 0:
            raise RuntimeError("clean/weak í˜ì–´ ì—†ìŒ. fileindex/severity í™•ì¸ í•„ìš”.")

        print(f"Dataset - found {len(self.pairs)} cleanâ€“weak pairs.")

    def _make_sample(self, row):
        img_path = row["filepath"]
        if not os.path.exists(img_path):
            raise FileNotFoundError(img_path)

        img = Image.open(img_path).convert("RGB")
        modality = row["dataset_norm"]
        label = int(row["binarylabel"])

        prompt = PROMPT_BY_DATASET.get(
            modality,
            "This is a medical image.\nQuestion: Does this image show normal anatomy or signs of disease?\n\n",
        )

        full_text = prompt
        if SYSTEM_PROMPT_NORMAL:
            full_text = SYSTEM_PROMPT_NORMAL + "\n" + prompt

        label_text = "normal" if label == 0 else "disease"

        return {
            "image": img,
            "input_text": full_text,
            "label_text": label_text,
            "label": label,
        }

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        clean_row, weak_row = self.pairs[idx]
        clean_sample = self._make_sample(clean_row)
        weak_sample  = self._make_sample(weak_row)
        return {
            "clean": clean_sample,
            "weak":  weak_sample,
        }

def make_clean_weak_collate_fn(processor, backend):
    def collate(batch):
        # ë¼ë²¨ í…ìŠ¤íŠ¸ ì¸ì½”ë”© í—¬í¼
        def encode_labels(label_texts):
            if hasattr(processor, "tokenizer") and processor.tokenizer is not None:
                enc = processor.tokenizer(
                    text=label_texts,
                    return_tensors="pt",
                    padding=True,
                )
                return enc["input_ids"]
            else:
                enc = processor(
                    text=label_texts,
                    return_tensors="pt",
                    padding=True,
                )
                return enc["input_ids"]

        def build_inputs(which):
            images, texts, label_texts, labels_int = [], [], [], []
            for item in batch:
                s = item[which]
                images.append(s["image"])
                texts.append(s["input_text"])
                label_texts.append(s["label_text"])
                labels_int.append(s["label"])

            # ğŸ”¹ Lingshu(Qwen2.5-VL) ì „ìš© ì²˜ë¦¬
            if backend == "lingshu":
                # ê° ìƒ˜í”Œë§ˆë‹¤ messages ë§Œë“¤ê³  chat í…ìŠ¤íŠ¸ ìƒì„±
                messages_list = []
                for img, txt in zip(images, texts):
                    messages_list.append(
                        [
                            {
                                "role": "user",
                                "content": [
                                    {"type": "image", "image": img},
                                    {"type": "text", "text": txt},
                                ],
                            }
                        ]
                    )

                chat_texts = [
                    processor.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=False,
                    )
                    for messages in messages_list
                ]

                # chat_texts ê¸¸ì´ == batch_size, images ê¸¸ì´ == batch_size
                model_inputs = processor(
                    text=chat_texts,
                    images=images,
                    padding=True,
                    return_tensors="pt",
                )

            # ğŸ”¹ InternVL ë“± ì¼ë°˜ ì¼€ì´ìŠ¤
            else:
                model_inputs = processor(
                    text=texts,
                    images=images,
                    return_tensors="pt",
                    padding=True,
                )

            labels_token = encode_labels(label_texts)
            labels_cls   = torch.tensor(labels_int, dtype=torch.long)

            out = dict(model_inputs)
            out["labels_token"] = labels_token
            out["labels_cls"]   = labels_cls
            return out

        return {
            "clean": build_inputs("clean"),
            "weak":  build_inputs("weak"),
        }

    return collate




# ============================================================
# 4. ê³µí†µ helper (train/val í•œ epoch)
# ============================================================
def to_device(batch_dict):
    out = {}
    for k, v in batch_dict.items():
        if isinstance(v, torch.Tensor):
            out[k] = v.to(device)
        else:
            out[k] = v
    return out

def last_hidden(outputs):
    hs = outputs.hidden_states[-1]  # (B, T, H)
    return hs[:, -1, :]             # (B, H)

def run_epoch(sp_model, loader, optimizer=None):
    train = optimizer is not None
    if train:
        sp_model.train()
    else:
        sp_model.eval()

    total_loss = 0.0
    total_ce   = 0.0
    total_cons = 0.0
    n_steps = 0

    with torch.set_grad_enabled(train):
        for batch in loader:
            clean = to_device(batch["clean"])
            weak  = to_device(batch["weak"])

            labels_clean = clean["labels_token"]
            labels_weak  = weak["labels_token"]

            def forward_one(mode_dict, labels_token):
                mode_dict = dict(mode_dict)
                
                # 1. ë¼ë²¨ ì„¤ì •
                mode_dict["labels"] = labels_token
                
                # 2. ë¶ˆí•„ìš”í•œ í‚¤ ì œê±° (DataLoaderì—ì„œ ì˜¬ë¼ì˜¨ ì„ì‹œ í‚¤ë“¤)
                mode_dict.pop("labels_token", None)
                mode_dict.pop("labels_cls", None)
                
                # ğŸ”¥ 3. Qwen/InternVL ì—ëŸ¬ ë°©ì§€: 
                # SoftPromptVLM ë‚´ë¶€ì—ì„œ inputs_embedsë¥¼ ë§Œë“¤ ê²ƒì´ë¯€ë¡œ, 
                # ì›ë³¸ input_idsëŠ” argumentsë¡œ ì§ì ‘ ë„˜ê¸°ë˜, 
                # ëª¨ë¸(sp_model)ì´ ë‚´ë¶€ì—ì„œ input_idsë¥¼ Noneìœ¼ë¡œ ì²˜ë¦¬í•˜ê²Œ í•´ì•¼ í•©ë‹ˆë‹¤.
                # (ìœ„ì˜ SoftPromptVLM ìˆ˜ì •ì‚¬í•­ì´ ì ìš©ë˜ì—ˆë‹¤ë©´ ì—¬ê¸°ì„œëŠ” ê·¸ëŒ€ë¡œ ë„˜ê²¨ë„ ë©ë‹ˆë‹¤.)
                
                outputs = sp_model(**mode_dict, output_hidden_states=True)
                return outputs

            out_c = forward_one(clean, labels_clean)
            out_w = forward_one(weak,  labels_weak)

            L_c = out_c.loss
            L_w = out_w.loss
            L_ce = w_c * L_c + w_w * L_w

            h_c = last_hidden(out_c)
            h_w = last_hidden(out_w)
            L_cons = lambda_w * F.mse_loss(h_c, h_w)

            loss = L_ce + L_cons

            if train:
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

            total_loss += loss.item()
            total_ce   += L_ce.item()
            total_cons += L_cons.item()
            n_steps += 1

    if n_steps == 0:
        return 0.0, 0.0, 0.0
    return total_loss / n_steps, total_ce / n_steps, total_cons / n_steps


# ============================================================
# 5. BACKENDë³„ë¡œ ìˆœì°¨ í•™ìŠµ (lingshu, internvl)
# ============================================================
SAVE_DIR = "./soft_prompt_ckpt"
os.makedirs(SAVE_DIR, exist_ok=True)

for BACKEND in BACKENDS:
    print("\n==============================")
    print(f"ğŸš€ Training backend: {BACKEND}")
    print("==============================")

    model_id = MODEL_ID_BY_BACKEND[BACKEND]

    # 1) base_model, processor ë¡œë“œ
    base_model, processor = load_backend(BACKEND, model_id)
    base_model.to(device)

    # 2) soft-prompt ë˜í¼ & optimizer
    keep_input_ids = (BACKEND == "lingshu")  # lingshu(Qwen2.5-VL)ë§Œ True
    sp_model = SoftPromptVLM(
        base_model,
        num_virtual_tokens=NUM_VIRTUAL_TOKENS,
        keep_input_ids=keep_input_ids,
    )
    sp_model.to(device)
    optimizer = AdamW(sp_model.soft_prompt.parameters(), lr=LR)

    # 3) Dataset / DataLoader (backendë³„ processor ì‚¬ìš©)
    train_ds = CleanWeakPairDataset(
        df=train_df,
        processor=processor,
        prompt_by_dataset=PROMPT_BY_DATASET,
        system_prompt=SYSTEM_PROMPT_NORMAL,
    )
    val_ds = CleanWeakPairDataset(
        df=val_df,
        processor=processor,
        prompt_by_dataset=PROMPT_BY_DATASET,
        system_prompt=SYSTEM_PROMPT_NORMAL,
    )

    # ê¸°ì¡´: collate_fn = make_clean_weak_collate_fn(processor)
    collate_fn = make_clean_weak_collate_fn(processor, BACKEND)


    train_loader = DataLoader(
        train_ds,
        batch_size=BATCH_SIZE,
        shuffle=True,
        collate_fn=collate_fn,
    )
    val_loader = DataLoader(
        val_ds,
        batch_size=BATCH_SIZE,
        shuffle=False,
        collate_fn=collate_fn,
    )

    # 4) í•™ìŠµ ë£¨í”„
    for epoch in range(EPOCHS):
        tr_loss, tr_ce, tr_cons = run_epoch(sp_model, train_loader, optimizer)
        val_loss, val_ce, val_cons = run_epoch(sp_model, val_loader, optimizer=None)

        print(
            f"[{BACKEND}] Epoch {epoch+1}/{EPOCHS} | "
            f"train: total={tr_loss:.4f}, CE={tr_ce:.4f}, Cons={tr_cons:.4f} | "
            f"val: total={val_loss:.4f}, CE={val_ce:.4f}, Cons={val_cons:.4f}"
        )

    # 5) soft_prompt ì €ì¥
    ckpt_path = os.path.join(
        SAVE_DIR,
        f"{BACKEND}_soft_prompt_prompt_tuning_900.pt"
    )
    torch.save(
        {
            "state_dict": sp_model.soft_prompt.state_dict(),
            "num_virtual_tokens": sp_model.num_virtual_tokens,
            "backend": BACKEND,
            "model_id": model_id,
        },
        ckpt_path,
    )
    print(f"âœ… Saved soft_prompt for {BACKEND} â†’ {ckpt_path}")

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del sp_model, base_model, processor, optimizer, train_ds, val_ds, train_loader, val_loader
    torch.cuda.empty_cache()
